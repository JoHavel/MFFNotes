\documentclass[12pt]{article}					% Začátek dokumentu
\usepackage{../../MFFStyle}							% Import stylu

\begin{document}

\begin{poznamka}
	Toto nejsou úplné zápisky z přednášky, toto je jen moje příprava k zápočtovému testu a později ke zkoušce.
\end{poznamka}

\section{Markovovy řetězce}

\begin{definice}[Markovův řetězec]
	Nechť $S$ je nejvýše spočetná množina. Posloupnost $\(X_t\)_{t=0}^∞$ náhodných veličin s oborem hodnot v $S$ je Markovův řetězec (s diskrétním časem, s diskrétním prostorem a časově homogenní) pokud pro každé $t ≥ 0$ a každé $a_0, …, a_{t+1} \in S$ platí
	$$ P(X_{t+1} = a_{t+1} | X_t = a_t \land … \land X_0 = a_0) = P(X_{t+1} = a_{t+1} | X_t = a_t) = p_{a_t, a_{t+1}}, $$
	pokaždé, když $P(X_t = a_t \land … \land X_0 = a_0) > 0$.

	Množině $S$ se říká stavy, budeme předpokládat, že jsou nějak (pevně) očíslované přirozenými čísly (resp. přirozenými čísly s 0). $p_{a_t, a_{t+1}}$ je pravděpodobnost přechodu ze stavu $a_t$ do stavu $a_{t+1}$
\end{definice}

\subsection{Přechody}

\begin{definice}[Přechodová matice]
	Matice $P$, jejíž prvek $p_{i, j}$ vyjadřuje pravděpodobnost přechodu ze stavu $i$ do stavu $j$.
\end{definice}

\begin{dusledek}
	Každý řádek přechodové matice má součet jeho prvků roven 1. Tj. $P·(1, …, 1)^T = (1, …, 1)^T$.
\end{dusledek}

\begin{definice}[Přechodový graf/diagram]
	Přechodový graf je ohodnocený orientovaný graf se smyčkami, jehož množina vrcholů je $S$. Hrana mezi vrcholy $i, j \in S$ vede právě tehdy, když $p_{i, j} > 0$ a má váhu $p_{i, j}$.
\end{definice}

\begin{definice}[Pravděpodobnostní rozdělení $X$]
	Nechť $\(X_t\)_{t=0}$ je Markovův řetězec. Pravděpodobnostní rozdělení $X_t$ budeme značit $\pi_i^{(t)} = P(X_t = i)$ pro každý stav $i \in S$, $t \in ®N_0$. $\pi^{(t)}$ pak značí řádkový vektor hodnot $\pi_i^{(t)}$.
\end{definice}

\begin{veta}
	Pro libovolný Markovův řetězec s pravděpodobnostním rozdělením $\pi$ a přechodovou maticí $P$ a libovolné $k≥0$
	$$ \pi^{(k)} = \pi^{(0)} · P^k. $$
	Dokonce obecněji $\pi^{(t + k)} = \pi^{(t)} P^k$.

	\begin{dukazin}
		$$ \forall m \in ®N: P(X_m = j) = \sum_{i \in S} P(X_{m-1} = i)·P(X_m = j | X_{m-1} = i), $$
		$$ \pi_j^{(m)} = \sum_{i \in S} \pi_i^{(m-1)} · P_{i, j}, $$
		$$ \pi^{(m)} = \pi^{(m-1)}·P. $$
	\end{dukazin}
\end{veta}

\begin{definice}[$k$-krokový přechod]
	$$ r_{i, j}(k) = P(\text{přechod z $i$ do $j$ za $k$ kroků}) = P(X_k = j | X_0 = i). $$
\end{definice}

\begin{dusledek}
	$$ r_{i, j}(k) = P(X_{t + k} = j | X_t = i). $$
\end{dusledek}

\begin{veta}[Chapman-Kolmogorov]
	Pro libovolný Markovův řetězec a libovolné $k, l \in ®N_0$ platí

	\begin{itemize}
		\item $r_{i, j}(k) = \(P^{(k)}\)_{i, j}$;
		\item $r_{i, j}(k + l) = \sum_{u \in S} r_{i, u}(k) r_{u, j}(l)$;
		\item $r_{i, j}(k + 1) = \sum_{u \in S} r_{i, u}(k) p_{u, j}$.
	\end{itemize}
\end{veta}

\subsection{Klasifikace stavů}

\begin{definice}[Dosažitelný stav]
	Pro stavy $i, j$ Markovova řetězce říkáme, že $j$ je dosažitelný z $i$ (píšeme $j \in A(i)$ nebo $i \rightarrow j$), pokud je nenulová pravděpodobnost, že začínaje v $i$ dosáhneme $j$ v konečném čase. Tedy
	$$ j \in A(i) ≡ \exists t \in ®N_0: P(X_t = j | X_0 = i) > 0. $$
\end{definice}

\begin{poznamka}
	Nevím, jestli na přednášce bylo $\exists t: P…$ nebo $P(\exists t: …) > 0$. Pokud se nepletu, je to ekvivalentní.
\end{poznamka}

\begin{dusledek}
	$j \in A(i)$ odpovídá existenci orientované cesty z $i$ do $j$ v přechodovém grafu.
\end{dusledek}

\begin{definice}[Komutující stavy]
	Říkáme, že stavy $i, j$ Markovova řetězce komutují, pokud $i \in A(j)$ a $j \in A(i)$. Píšeme $i \leftrightarrow j$.
\end{definice}

\begin{veta}
	Pro libovolný Markovův řetězec je relace $\leftrightarrow$ (na $S$) ekvivalence.
\end{veta}

\begin{definice}[Ireducibilní Markovův řetězec]
	Markovův řetězec se nazývá ireducibilní, pokud $\forall i, j \in S: i \leftrightarrow j$.
\end{definice}

\begin{definice}[Rekurentní stav]
	Stav $i \in S$ Markovova řetězce se nazývá rekurentní, pokud $\forall j \in A(i): i \in A(j)$.
\end{definice}

\begin{definice}[Transientní stav]
	Stav $i \in S$ Markovova řetězce se nazývá transientní (význam: dočasný, přechodný, pomíjivý), pokud není rekurentní.
\end{definice}

\begin{veta}
	Pro stav $i \in S$ Markovova řetězce označme $f_{ii} = P(\exists t \in ®N: X_t = i | X_0 = i)$. Ať $|S| < ∞$. Potom, když $f_{ii} = 1$, tak je stav rekurentní, pokud $f_{ii} < 1$, tak je transientní.

	\begin{dukazin}[Transientní]
		Označme $j$ to $j \in A(i)$, pro které $i \notin A(j)$. Potom $P(\exists t \in ®N: X_t = j|X_0 = i) ≠ 0$ a zřejmě $P(\exists t \in ®N\ \forall 0 < t_1 < t: X_t = j \land X_{t_1} ≠ i | X_0 = i) ≠ 0$ a $P(\exists t_2 > t: X_{t_2} = i | X_t = j) = 0$, tedy $f_{ii} ≠ 1$.
	\end{dukazin}

	\begin{dukazin}[Rekurentní, na přednášce nebyl celý, moje vize:]
		Nechť $m = \min_{j \in A(i)} P(\exists \tilde t < t: X_{\tilde t} = i | X_0 = j)$. Pro dostatečně velké $t$ (maximum přes všechny časy z definice rekurentního stavu) je $m > 0$. To znamená, že $\sum_{j \in A(i), j ≠ i} \pi^{(t)}_j ≤ (1 - m)·\sum_{j \in A(i), j ≠ i} \pi^{(0)}_j$ (předpokládaje, že $p_{i,i} = 1$, protože při libovolném navštívení $i$ jsme vyhráli). Tedy (stále předpokládaje $p_{i, i} = 1$)
		$$ \lim_{n \rightarrow ∞} \sum_{j \in A(i), j ≠ i} \pi^{(n·t)} ≤ \lim_{n \rightarrow ∞} (1 - m)^n·\sum_{j \in A(i), j ≠ i} \pi^{(0)} = 0·… = 0. $$
		Ale pokud jsme začínali uvnitř $A(i)$ (což po rozutečení se z $i$ rozhodně), tak $\sum_{j \in S \setminus A(i)} \pi^{(·)}_j = 0$, tedy $\pi^{(·)}_1 \rightarrow 1$.
	\end{dukazin}
\end{veta}

\begin{definice}[Počet návštěv]
	Pro stav $i \in S$ Markovova řetězce označme náhodnou veličinu $V_i$ s oborem hodnot v $®N_0^*$ počet návštěv $i$, tedy $V_i = |\{t | X_t = i\}|$.
\end{definice}

\begin{veta}
	Stav $i \in S$ Markovova řetězce je rekurentní $\implies P(V_i = ∞ | X_0 = i) = 1$. $i$ je transientní, pokud $V_i|_{X_0 = i} \sim Geo(1 - f_{ii})$.
\end{veta}

\begin{definice}[Stacionární rozložení]
	Nechť $\pi$ je pravděpodobnostní rozložení na stavech $S$ Markovova řetězce. Řekneme, že $\pi$ je stacionární rozložení, pokud $\pi·P = \pi$, kde $\pi$ považujeme za řádkový vektor.
\end{definice}

\begin{dusledek}
	Pokud $\pi^{(0)}$ je stacionární rozložení, pak $\forall k \in ®N_0: \pi^{(k)} = \pi^{(0)}$.
\end{dusledek}

\begin{definice}[Periodický stav, periodický Markovův řetězec, aperiodický …]
	Stav $i \in S$ Markovova řetězce je periodický, pokud $\exists \Delta \in ®N \setminus \{1\}$:
	$$ P(X_t = i | X_0 = i) > 0 \implies \Delta | t. $$

	Markovův řetězec se nazývá periodický, pokud jsou všechny jeho stavy periodické.

	Stav nebo Markovův řetězec se nazývá aperiodický, pokud není periodický.
\end{definice}

\begin{veta}
	Buď $(X_t)_{t=0}^∞$ Markovův řetězec, který je ireducibilní, aperiodický a $|S| < ∞$. Potom $\exists \pi$ stacionární rozložení a
	$$ \forall j\ \forall i \lim_{k \rightarrow ∞} r_{i, j}(k) = \pi_j; $$
	navíc $\pi$ je jednoznačné řešení $\pi·P = \pi$ a $\pi·(1, …, 1)^T = 1$.
\end{veta}

\begin{definice}[Absorbující stav]
	Stav $a \in S$ Markovova řetězce je absorbující, pokud $p_{a, a} = 1$.
\end{definice}

\begin{definice}[Čas absorbování]
	Předpokládejme $A \subseteq S$ neprázdnou množinu absorbujících stavů Markovova řetězce a BÚNO $0 \in A$. Pro každý stav $i \in S$ definujeme $\mu_i$ jako střední hodnotu času absorbování z $i$, tedy
	$$ \mu_i = ®E(T | X_0 = i), \qquad T = \min\{t: X_t \in A\}. $$

	Dále $a_i$ buď pravděpodobnost, že začínaje ve stavu $i$ skončíme v stavu $0$.
	$$ a_i = P(\exists t: X_t = 0 | X_0 = i). $$
\end{definice}

\begin{veta}
	Pravděpodobnosti $a_i$ jsou jednoznačné řešení
	$$ a_0 = 1, \qquad a_i = 0, \quad 0 ≠ i \in A, \qquad a_i = \sum_{j \in S} p_{i, j} a_j, \quad i \in (S \setminus A) \cup \{0\}. $$

	\begin{dukazin}
		TODO? Jednoduchý, větou o úplné pravděpodobnosti. %3. přednáška, cca 0:25
	\end{dukazin}
\end{veta}

\begin{veta}
	Střední hodnoty času ($\mu_i$) jsou jednoznačné řešení
	$$ \mu_i = 0, \quad i \in A, \qquad \mu_i = 1 + \sum_{j \in S} p_{i, j} \mu_j, \quad i \in S \setminus A. $$

	\begin{dukazin}
		TODO? Jednoduchý, větou o úplné střední hodnotě. % Tenhle na přednášce asi ani nebyl.
	\end{dukazin}
\end{veta}

\section{SAT}
\begin{definice}[$k$-SAT]
	Je konjukce ($\phi$) $l$ klauzulí (= disjunkce nejvýše $k$ literálů = proměnná nebo její negace) splnitelná (vhodným dosazením ano/ne za proměnné)? (Proměnné označme $x_1, …, x_n$.)
\end{definice}

\begin{definice}[Algoritmus řešení 2-SAT]
	Začneme z libovolně přiřazenými proměnnými (např. všechny ne). Následně $2·m·n^2$-krát (pro zvolené $m \in ®N$) zopakujeme: pokud je vše splněno, vyhráli jsme; jinak zvolíme libovolně nesplněnou klauzuli a z ní změníme náhodně proměnnou a znegujeme jí (tím jsme danou klauzuli splnili). Pokud po $2·m·n^2$ krocích není hotovo, pak vrátíme ne.
\end{definice}

\begin{tvrzeni}
	Pravděpodobnost špatného výsledku je menší než $\frac{1}{nm}$.

	\begin{dukazin}
		Předpokládejme, že $\phi(s_1, …, s_n)$ je pravdivá a položme $X_t = |\{x_i^t = s_i\}$. Tedy pokud $X_t = n$, tak jsme našli splnění $\phi$.

		Pokud $X_t = 0$, pak $X_{t + 1} = 1$. Pokud $0 < X_t < n$, pak ve vybrané klauzuli máme minimálně jednu ze dvou proměnných špatně ($x_i ≠ s_i$). Když změníme správnou, tak $X_{t + 1} = X_t + 1$. Pokud zvolíme druhou, tak ona mohla být také správně, takže $X_{t + 1} = X_t ± 1$.

		Tím dostáváme Markovův řetězec tvaru $n+1$ dlouhé cesty, kde pravděpodobnost cesty doprava je alespoň $1 / 2$. Tento řetězec jsme (prý) už analyzovali, vyjde nám, že střední hodnota příchodu do posledního vrcholu je menší než $n^2$.

		Tím nám z Markovovy nerovnosti vychází, že $P(T > 2mn^2) ≤ \frac{1}{2m}$, kde $T$ je nejmenší tak, že $X_T = n$.
	\end{dukazin}
\end{tvrzeni}

\begin{tvrzeni}
	Pravděpodobnost špatného výsledku je menší než $\frac{1}{2^m}$.

	\begin{dukazin}
		$m$-krát zopakujeme postup pro „$m=1$“ (začátek volíme libovolně, takže je nám jedno, že předchozí iterace nám dala nějaký stav, ze kterého pokračujeme).
	\end{dukazin}
\end{tvrzeni}

\begin{poznamka}
	Když toto aplikujeme na 3-SAT, tak budeme mít problém s tím, že pravděpodobněji půjdeme doleva místo doprava. Tudíž musíme něco zlepšit.
\end{poznamka}

\begin{definice}[Algoritmus pro řešení 3-SAT]
	Zopakujeme $2·2·3^{n / 2}$ krát: náhodně zvolíme začátek a $n / 2$-krát zopakujeme krok z 2-SATu.
\end{definice}

\begin{tvrzeni}
	Špatnou odpověď dá tento algoritmus s pravděpodobností $\frac{1}{2}$.

	\begin{dukazin}
		V každém z $2·2·3^{n / 2}$ kroků (začínáme náhodně, tedy $X_0$ má binomické rozdělení)
		$$ P(win) = P(X_0 ≥ n / 2) · P(win | X_0 ≥ n / 2) ≥ \frac{1}{2} 3^{-n / 2}. $$

		Tedy střední hodnota opakování vnějšího cyklu je $\frac{1}{p} = 2·3^{n / 2}$. A my víme, že $P(T > 2·2·3^n) ≤ \frac{®E T}{2·2·3^n} ≤ \frac{1}{2}$. 
	\end{dukazin}
\end{tvrzeni}

\section{Bayesovská statistika}

\subsection{Postup}

\begin{definice}[Parametr hledaného rozdělení]
	Hledáme rozdělení s parametrem $\Theta$, který budeme považovat za náhodnou veličinu.
\end{definice}

\begin{definice}[Apriorní rozdělení]
	Nejprve vybereme apriorní rozdělení s pmf (probability mass function) $p_\Theta(\vartheta)$ nebo pdf (probability density function) $f_\Theta(\vartheta)$ náhodné veličiny $\Theta$ nezávisle na datech.
\end{definice}

\begin{definice}[Statistický model]
	Potom zvolíme statistický model $p_{X|\Theta}(x|\vartheta)$ (nebo $f_{X|\Theta}(x|\vartheta)$), který popisuje jak jsou (věříme, že jsou) rozděleny data, pokud je $\Theta$ rovno nějakému konkrétnímu $\vartheta$.
\end{definice}

\begin{definice}[Posteriorní rozdělení]
	Poté, co pozorujeme $X = x$ (více měření považujeme za pozorování jednoho $X = x$ z vícedimenzionálního rozdělení) spočítáme posteriorní rozdělení $f_{\Theta | X}(\vartheta | x)$.
\end{definice}

\begin{poznamka}
	Nakonec najdeme, co potřebujeme vědět, například $a, b$ tak, aby $P(a ≤ \Theta ≤ b | X = x) = \int_a^b f_{(\Theta | X)}(\vartheta | x) d\vartheta ≥ 1 - \alpha$.
\end{poznamka}

\subsection{Bayesova věta}

\begin{veta}[Bayesova pro obě diskrétní]
	Nechť $X$, $\Theta$ jsou diskrétní náhodné veličiny, pak
	$$ p_{\Theta | X}(\vartheta | x) = \frac{p_{X|\Theta}(x|\vartheta) p_\Theta(\vartheta)}{\sum_{\vartheta' \in \im \Theta \setminus \{p_\Theta(\vartheta') = 0\}} p_{X|\Theta}(x|\vartheta')p_\Theta(\vartheta')}. $$
\end{veta}

\begin{veta}[Bayesova pro obě spojitá]
	Nechť $X$, $\Theta$ jsou spojité náhodné veličiny, pak
	$$ f_{\Theta | X}(\vartheta | x) = \frac{f_{X|\Theta}(x|\vartheta) f_\Theta(\vartheta)}{\int_{\vartheta' \in \im \Theta \setminus \{f_\Theta(\vartheta') = 0\}} f_{X|\Theta}(x|\vartheta')f_\Theta(\vartheta')}. $$
\end{veta}

\begin{veta}[Bayesova pro diskrétní a spojité]
	Nechť $X$ je diskrétní a $\Theta$ spojitá náhodná veličina, pak
	$$ f_{\Theta | X}(\vartheta | x) = \frac{p_{X|\Theta}(x|\vartheta) f_\Theta(\vartheta)}{\int_{\vartheta' \in \im \Theta \setminus \{f_\Theta(\vartheta') = 0\}} p_{X|\Theta}(x|\vartheta')f_\Theta(\vartheta')}. $$
\end{veta}

\subsection{Bodové odhady}

\begin{definice}[MAP – maximum a-posteriori]
	Zvolíme modus $\Theta$.

	\begin{poznamkain}
		Tj. maximum $p_{\Theta|X}(\vartheta|x)$, resp $f_{\Theta|X}(\vartheta|x)$.
	\end{poznamkain}
\end{definice}

\begin{definice}[LMS – least mean square]
	Zvolíme střední hodnotu $\Theta$, tedy $®E(\Theta | X = x)$.

	\begin{poznamka}
		Dostaneme nestranný bodový odhad, který minimalizuje $®E((\Theta - ·)^2 | X = x)$.
	\end{poznamka}
\end{definice}

\begin{poznamka}[Medián]
	Obdobně, když vezmeme medián (tj. $m$ tak, že $P(\Theta ≤ m | X = x) = \frac{1}{2}$), tak minimalizujeme $®E((\Theta - ·) | X = x)$, tento přístup však nebudeme dále používat.
\end{poznamka}

TODO? (Zbytek B. statistiky, speciálně Bayesův klasifikátor.) % Zbytek 4. přednášky až konec 6. přednášky.

\section{Stochastické procesy}

\begin{poznamka}
	I Markovovy řetězce jsou vlastně stochastický proces.
\end{poznamka}

\subsection{Bernoulliho proces}

\begin{definice}[Bernoulliho proces]
	Bernoulliho proces (s parametrem $p$), píšeme $Bp(p)$, je posloupnost nezávislých náhodných veličin $(X_t)_{t=1}^∞$, kde $X_t \sim Ber(p)$, tedy $p(X_t = 1) = p$ a $p(X_t = 0) = 1 - p$, $\forall t \in ®N$.
\end{definice}

\begin{dusledek}
	$$ \{X_t\}_{t = 1}^∞ \sim Bp(p) \implies \{X_t\}_{t = k}^∞ \sim Bp(p), \forall k \in ®N. $$

	$$ \{X_t\}_{t = 1}^∞ \sim Bp(p) \implies \{X_t\}_{t = N}^∞ \sim Bp(p), $$
	kde $N$ je náhodná veličina závisející pouze na minulosti.
\end{dusledek}

\begin{definice}[Čas prvního úspěchu, čas $k$-tého]
	$$ T := \min\{t | X_t = 1\}, \qquad T_k := \min\{t \middle| \sum_{s = 1}^t X_s = k\}. $$
\end{definice}

\begin{dusledek}
	$$ T \sim Geom(p), \qquad ®E[T] = \frac{1}{p}, \qquad \var T = \frac{1 - p}{p^2}. $$
\end{dusledek}

\begin{definice}[Doba čekání]
	$$ L_k := T_k - T_{k-1}, \qquad (T_0 = 0). $$
\end{definice}

\begin{dusledek}
	$$ L_k \sim T \sim Geom(p). $$

	\begin{dukazin}
		Restartujeme Bernoulliho proces v $T_{k - 1}$.
	\end{dukazin}
\end{dusledek}

\begin{dusledek}
	$$ T_k = \sum_{i = 1}^k L_i. $$
	$$ ®E [T_k] = \sum_{i=1}^k ®E L_i = \frac{k}{p}, \qquad \var T_k = \sum_{i=1}^k \var L_i = k·\frac{1 - p}{p^2}. $$
	$$ p(T_k = t) = \binom{t - 1}{k - 1}·p^k·(1 - p)^{t - k}, \qquad \chi(T_k = t) \sim Pas(p, k), $$
	kde $Pas(p, k)$ je tzv. Pascalovo rozdělení (definované právě $p(T_k = t) = …$ výše), také nazývané negativní binomické.
\end{dusledek}

\begin{veta}[Spojování Bernoulliho procesů]
	Mějme $\{X_t\}_{t=1}^∞ \sim Bp(p)$ a $\{Y_t\}_{t=1}^∞ \sim Bp(q)$, pak $\{X_t \lor Y_t\}_{t=1}^∞ \sim Bp(p + q - pq)$.
\end{veta}

\begin{veta}[Rozdělování Bernoulliho procesů]
	Mějme $\{Z_t\}_{t = 1}^∞ \sim Bp(p)$. Potom $\{Z_t · Y_t\}_{t=1}^∞ \sim Bp(p·q)$, kde $Y_t \sim Ber(q)$ jsou navzájem nezávislé (a nezávislé na $Z_t$).
\end{veta}

\subsection{Poissonův proces}

\begin{definice}[Poissonův proces]
	Definujme časy příchodů jako reálná čísla: $0 < T_1 < T_2 < T_3 < …$. Po Poissonově procesu požadujeme:

	\begin{enumerate}
		\item Pro každou délku intervalu $\tau$ chceme, aby pravděpodobnost $k$ příchodů v tomto intervalu byla stejná, označme ji $p(k, \tau)$.
		\item Počet příchodů v intervalu $[a, b]$ je nezávislý na počtu příchodů v $[0, a]$.
		\item $p(0, \tau) = 1 - \lambda \tau + o(\tau)$, $p(1, \tau) = \lambda \tau + o(\tau)$ ($\implies p(k, \tau) = o(\tau)$, $\forall k ≥ 2$).
	\end{enumerate}

	Poissonův proces je tedy posloupnost náhodných reálných veličin $0 < T_1 < T_2 < T_3 < …$, která splňuje tyto 3 body.
\end{definice}

\begin{definice}[Počet příchodů do času $t$]
	$$ N_t := \max {k | T_k ≤ t} $$
\end{definice}

\begin{veta}
	$$ N_t \sim Pois(\lambda·t), \qquad p(N_t = k) = e^{-\lambda·t} \frac{(\lambda·t)^k}{k!}. $$
	
	\begin{dukazin}
		Rozdělme si interval $[0, t]$ na $l$ intervalů pro nějaké $l$ velké. Pak délka jednoho intervalu je $\frac{t}{l}$, $p\(1, \frac{t}{l}\) = \frac{\lambda·t}{l} + o\(\frac{t}{l}\)$ a $p\(k, \frac{t}{l}\) = o\(\frac{t}{l}\)$. $o\(\frac{t}{l}\)$ zanedbáme, tedy máme Binomické rozdělení s parametry $l$ a $\frac{\lambda · t}{l}$, což pro rostoucí $l$ vede k Poissonovu rozdělení s parametrem $\lambda·t$. Tedy
		$$ p(N_t = k) = e^{-\lambda·t} \frac{(\lambda·t)^k}{k!}. $$
	\end{dukazin}
\end{veta}

\begin{definice}[Čekání na další příchod]
	$$ L_k := T_k - T_{k-1}. $$
\end{definice}

\begin{dusledek}
	$$ p(L_k ≥ t) = p(0, t) = e^{-\lambda·t}, \qquad p(L_k ≤ t) = 1 - p(L_k ≥ t) = 1 - e^{-\lambda·t}. $$
	$$ L_k \sim Exp(\lambda). $$
\end{dusledek}

\begin{dusledek}
	$$ ®E T_k = \sum_{i = 1}^k ®E L_i = k·\frac{1}{\lambda}. $$
	$$ \var T_k = \sum_{i=1}^k \var L_i = k·\frac{1}{\lambda^2}. $$
	$$ f_{T_k}(t) = \frac{\lambda^k t^{k-1} e^{-\lambda·t}}{(k-1)!} $$
\end{dusledek}

\begin{veta}[Rozdělování Poissonových procesů]
	Mějme $0 < T_1 < T_2 < …$ Poissonův proces s parametrem $\lambda$ a každý příchod nezávisle s pravděpodobností $p$ ponechejme. Pak nová $0 < T_1' < T_2' < …$ jsou Poissonův proces s parametrem $\lambda·p$. Odstraněné $0 < \tilde T_1 < \tilde T_2 < …$ jsou Poissonův proces s parametrem $\lambda·(1 - p)$. A tyto procesy jsou na sobě nezávislé.

	\begin{dukazin}
		$$ p_p(k, \tau) = \sum_{n=k}^∞ p(n, \tau)·P(Bin(n, p) = k). $$
		Následně se ověří podmínky Poissonova procesu (na přednášce ukázán trochu zjednodušený výpočet).

		Nezávislé $\Leftrightarrow$ $P(X = k \land Y = l) = P(X = k)·P(Y = l)$. Následně jsme ověřili dosazením.
	\end{dukazin}
\end{veta}

\begin{veta}[Spojování Poissonových procesů]
	Nechť $0 < T_1 < T_2 < …$ a $0 < S_1 < S_2 < …$ jsou Poissonovy procesy s parametry $\lambda$, $\kappa$. Potom jejich sjednocením získáme Poissonův proces $0 < R_1 < R_2 < …$ s parametrem $\lambda + \kappa$. (Případně můžeme spojovat i libovolně mnoho Poissonových procesů do Poissonova procesu s parametrem rovným součtu parametrů původních.)

	\begin{dukazin}
		$$ p(R_1 > t) = P(T_1 > t \land S_1 > t) = P(T_1 > t)·P(S_1 > t) = e^{-\lambda t}·e^{-\kappa t} = e^{-(\lambda + \kappa) t}. $$
		Následně restartujeme procesy v $R_1$ a začínáme nanovo :)
	\end{dukazin}
\end{veta}

\section{Balls and bins}

\begin{definice}[Narozeninový paradox]
	$k$ lidí, jaká je pravděpodobnost, že dva lidé mají narozeniny ve stejný den?

	\begin{poznamkain}
		Řešení:
		$$ P(\text{každý v jiný den}) = \(1 - \frac{1}{365}\)·\(1 - \frac{2}{365}\)·…·\(1 - \frac{k - 1}{365}\) ≈ $$
		$$ (e^{-x} ≈ 1 - x) \qquad ≈ \prod_{i=1}^{k-1} e^{-\frac{i}{365}} = e^{-\sum_{i=1}^{k-1} \frac{i}{365}} = e^{-\frac{k·(k-1)}{2·365}}. $$
	\end{poznamkain}
\end{definice}

\begin{definice}[Balls and bins]
	Máme $m$ kuliček, které rozdělíme do $n$ příhrádek.
	
	\begin{prikladyin}
		Můžeme se ptát na:
		\begin{itemize}
			\item narozeninový paradox;
			\item \# kuliček v první příhrádce ($\sim Bin(m, 1 / n)$);
			\item první příhrádka prázdná ($(1 - 1 / n)^m ≈ e^{- m / n}$);
			\item \# prázdných příhrádek ($®E = n·(1 - 1 / n)^m ≈ n·e^{-m / n}$);
			\item průměrný počet kuliček v příhrádce ($m / n$);
			\item maximální počet kuliček v příhrádce (následující věta);
			\item …
		\end{itemize}
	\end{prikladyin}
\end{definice}

\begin{veta}
	Pokud $m = n$ je velké, $M := \frac{3 \log n}{\log \log n}$, pak
	$$ P(\text{maximální počet kuliček} ≥ M) < \frac{1}{n}. $$

	\begin{dukazin}
		$$ P(\text{počet kuliček v příhrádce 1} ≥ M) ≤ P(Bin(n, 1 / n) = M) = $$
		$$ = \binom{n}{M}\frac{1}{n^M}\(1 - \frac{1}{n}\)^{n - M} < \binom{n}{M} \frac{1}{n^M} = \frac{n·(n - 1)·…·(n - (M - 1))}{M! · n^M} < \frac{1}{M!} < \(\frac{e}{M}\)^M. $$

		$$ P(\text{\# kuliček v nějaké příhrádce} ≥ M) ≤ \sum_{i=1}^m P(\text{\# kuliček v příhrádce $i$} ≥ M) = n·\(\frac{e}{M}\)^M. $$
		Chtěli bychom $n·\(\frac{e}{M}\)^M < \frac{1}{n}$. Tedy přidáme logaritmus:
		$$ \log n + M·(1 - \log M) < - \log n $$
		$$ 2\log n + \frac{3 \log n}{\log \log n} (1 - \log 3 - \log \log n + \log \log \log n) < 0 $$
		$$ -(\log n) · \(1 - 3\frac{1 - \log 3}{\log \log n} - 3·\frac{\log \log \log n}{\log \log n}\) < 0. $$
		A jelikož $\frac{\log x}{x} \rightarrow 0$ pro $x \rightarrow ∞$, tak pro dostatečně velká $n$ nerovnost platí.
	\end{dukazin}
\end{veta}

\begin{dusledek}[Bucketsort]
	Chceme setřídit $n = 2^k$ $l$-bitových („náhodných“) čísel. Rozdělíme čísla na prvních $k$ bitů ($b(x)$) a zbylých $l - k$ bitů. Potom za prvé roztřídíme čísla podle $b(x)$ do příhrádek ($1, …, 2^k$). Následně setřídíme každou příhrádku (např. bubblesortem) v kvadratickém čase. Nakonec slijeme příhrádky dohromady.

	Střední hodnota složitosti tohoto algoritmu je lineární.

	\begin{dukazin}
		První krok je lineární v $n$, stejně tak třetí. Po prvním kroku bude \# kuliček v $i$-té příhrádce $\sim Bin(n, 1 / n)$. Tedy složitost (ve střední hodnotě) kroku dva bude ($c$ je konstanta z bubblesortu)
		$$ ®E \sum_{i=1}^n c·X_i^2 = \sum_{i=1}^n c·®E (X_i^2) = n·c·\(\var X_i + (®E X)^2\) < 2n·c. $$
	\end{dukazin}
\end{dusledek}

\begin{dusledek}[Hešování]
	Chceme $n$ objektů (např. řetězců) ukládat tak, aby šlo rychle hledat. Předpokládáme, že máme hashovací funkci (zobrazení z objektů do $[0, m-1] \cap ®N$), která je „náhodná“.

	Pokud je přibližně $n < \sqrt{m}$, potom pravděpodobnost kolize (2 objekty mají stejný hash) je přibližně $\frac{1}{2}$ z narozeninového paradoxu.

	Pokud je $m = n$ dostatečně velké, pak pravděpodobnost, že maximální počet objektů v příhrádce překoná $M := \frac{3 \log n}{\log \log n}$ je menší než $\frac{1}{n}$ z předchozí věty.

	Očekávaný čas na nalezení prvku je ve všech případech $\frac{n}{m}$, neboť očekávaný počet objektů v příhrádce je $\frac{n}{m}$.

	Maximální čas nalezení bude pro $n = m$ dostatečně velká, nejvýše $M$ s pravděpodobností větší než $1 - \frac{1}{n}$. (Moc lépe to nejde kvůli následující větě.)
\end{dusledek}

\begin{veta}
	Za předpokladu dostatečně velkého $m = n$ a $M_2 = \frac{\log n}{\log \log n}$ je
	$$ P(\text{maximální počet kuliček} ≥ M_2) < \frac{1}{n}. $$
\end{veta}

\begin{definice}[Značení]
	$$ X_i^{(m)} = \# \text{ kuliček v $i$-té příhrádce}. $$
	To znamená $(X_1^{(m)}, …, X_n^{(m)})$ má multinomické rozdělení, tj. (pro $\sum k_i = m$, $0 ≤ k_i ≤ m$)
	$$ P\(X_1^{(m)} = k_1, …, X_n^{(m)} = k_n\) = \binom{m}{k_1, …, k_n} · \frac{1}{n^m} = \frac{m!}{k_1!·…·k_n!} · \frac{1}{n^m}. $$

	Také to znamená, že $X_i^{(m)}$ má rozdělení $Bin(m, 1 / n)$, což je přibližně $Pois(m / n)$.
\end{definice}

\begin{veta}
	Nechť $m, n \in ®N$, $Y_1^{(k)}, …, Y_n^{(k)}$ jsou nezávislé stejně rozdělené veličiny s rozdělením $Pois(k / n)$ a $X_i^{(m)}$ jako v předchozím. Pak rozdělení $X_i^{(m)}$ je shodné s rozdělením $Y_i^{(k)}$, pokud $\sum_{i=1}^n Y_i^{(k)} = m$.

	\begin{dukazin}
		Mějme $k_1 + … + k_n = m$ a $0 ≤ k_i ≤ m$, potom chceme
		$$ P\(X_1^{(m)} = k_1, …, X_n^{(m)} = k_n\) = P_X = P_Y = P\(Y_1^{(k)} = k_1, …, Y_n^{(k)} = k_n | \sum Y_i^{(k)} = m\). $$

		$$ P_X = \binom{m}{k_1, …, k_n} · \frac{1}{n^m}. \qquad P_Y = \frac{P(… |)}{P(| …)} = \frac{A}{B}. $$
		$$ A = P(Y_1^{(k)} = k_1) · … · P(Y_n^{(k)} = k_n) = e^{-\frac{k}{n}}·\frac{\(\frac{k}{n}\)^{k_1}}{k_1!} · … · e^{-\frac{k}{n}}·\frac{\(\frac{k}{n}\)^{k_n}}{k_n!} = e^{-k} · \(\frac{k}{n}\)^m · \frac{1}{k_1! · … · k_n!}. $$
		$$ \sum_{i=1}^n Y_i^{(k)} \sim Pois\(\frac{k}{n} + … + \frac{k}{n}\) = Pois(k) \implies B = e^{-k} \frac{k^m}{m!}. $$
	\end{dukazin}
\end{veta}

\begin{veta}
	Buďte $X$, $Y$ jako v předchozí větě a $f(x_1, …, x_n) ≥ 0$. Potom $®E f(X_1^{(m)}, …, X_n^{(m)}) ≤ ®E f(Y_1^{(k)}, …, Y_n^{(k)})·e·\sqrt{k}$.

	Navíc pokud je pravá strana monotónní v $m$, pak můžeme $e·\sqrt{k}$ nahradit $2$.

	\begin{dukazin}
		$$ ®E f(Y_1^{(k)}, …, Y_n^{(k)}) = \sum_{i=0}^∞ ®E \(f(Y_1^{(k)}, …, Y_n^{(k)}) | \sum_{j=1}^n Y_j^{(k)} = i\)·P\(\sum_{j=1}^n Y_j^{(k)} = i\) ≥ $$
		$$ ≥ \(f(Y_1^{(k)}, …, Y_n^{(k)}) | \sum_{j=1}^n Y_j^{(k)} = m\)·P\(\sum_{j=1}^n Y_j^{(k)} = m\) = $$
		$$ = ®E f(X_1^{(m)}, …, X_n^{(m)}) · P\(\sum_{j=1}^n Y_j^{(k)} = m\) = ®E f(X_1^{(m)}, …, X_n^{(m)}) · e^{-k} \frac{k^m}{m!} ≥ $$
		$$ ≥ ®E f(X_1^{(m)}, …, X_n^{(m)}) · \frac{1}{e\sqrt{k}}. $$

		TODO!!! % 10. přednáška cca 0:30

		Totéž provedeme pro monotónní pravou stranu, jen budeme odhadovat lepší pravděpodobností? (TODO?)
	\end{dukazin}
\end{veta}

\begin{dukaz}[Předpředchozí věty]
	Z předchozí věty (aplikované na $f(x_1, …, x_n) = (\max\{x_1, …, x_n\} < M)$) nám stačí dokázat, že
	$$ P(\max \{Y_1^{(k)}, …, Y_n^{(k)})\} < M) ≤ \frac{1}{e·\sqrt{k}·n}. $$

	$$ P(…) = P(Y_1^{(k)} < M) · … · P(Y_n^{(k)} < M) ≤ (1 - P(Y_1^{(k)} = M)) · … · (1 - P(Y_n^{(k)} = M)) = $$
	$$ = \(1 - e^{-1} \frac{1^M}{M!}\)^n ≈ \(e^{-\frac{1}{e·M!}}\)^n ≤ e^{-\frac{n}{e·M!}} < \frac{1}{n^2}, $$
	neboť to je totéž jako
	$$ \frac{1}{e·M!} > 2 \log n, $$
	což spočítáme pomocí odhadu $M! ≤ M · (M / e)^M$.
\end{dukaz}

\section{Neparametrická statistika}
\begin{definice}[Neparametrická statistika]
	Nemáme model (rozdělení závisející na parametru).
\end{definice}

TODO (Permutační test) % Konec 10. přednášky

\begin{definice}[Permutační test]
	Mějme data $x_1, …, x_n$ a $y_1, …, y_m$ (např. testovací a kontrolní vzorek). Dále mějme $f$, které rozhoduje, zda dané $z_1, …, z_{m+n}$ splňuje nulovou hypotézu.
	$$ ©F := \{f(\pi(z))\}_{\pi \in S_{n + m}} $$
	$p$-hodnota je podíl prvků souboru $©F$, které splňují nulovou hypotézu. Nulovou hypotézu zamítneme, pokud je tento podíl menší než $\alpha$.

	(Požadujeme, aby za nulové hypotézy byla pravděpodobnost každého prvku $®F$ stejná.)
\end{definice}

\begin{definice}[Permutační test ++]
	Pokud nemůžeme počítat $f$ pro všechny $\pi \in S_{n+m}$, nasamplujeme $©F^* \subset ©F$.
\end{definice}

\begin{definice}[Znamínkový test]
	$X_1, …, X_n$ nezávislé náhodné veličiny z neznámého spojitého rozdělení symetrické podle střední hodnoty. Nulová hypotéza je, že střední hodnota je 0.

	Nechť $Y_i = \sgn(X_i) = +1$ nebo $0$ (pozor, ne $-1$). Potom při předpokladu nulové hypotézy $Y = \sum_{i=1}^n Y_i \sim Binom(n, \frac{1}{2})$. Tedy nulovou hypotézu zamítneme, pokud $Y ≤ Y_{\alpha / 2}$ nebo $Y > Y_{1 - \alpha / 2}$, kde $P(Binom(n, \frac{1}{2}) < Y_x) = x$.
\end{definice}

\begin{definice}[Pair test]
	Mějme data, která jsou přirozeně v párech (např. hodnota před a po vylepšení algoritmu) a mějme nějakou hypotézu, kterou můžeme testovat po prvcích (např. jestli se průměr nových a starých hodnot shoduje, což můžeme testovat jako „jestli je průměr rozdílů hodnot 0“). Potom se můžeme na pár dívat jako na jeden prvek.
\end{definice}

\begin{definice}[Wilcoxonův test znamínka hodnosti]
	$X_1, …, X_n$ nezávislé náhodné veličiny z neznámého spojitého rozdělení symetrické podle střední hodnoty. Nulová hypotéza je, že střední hodnota je 0.
	
	Hodnost ($\rank, r_i$) je pořadí v seřazení $|X_i|$ (místo sdíleného pořadí vezmeme průměr sdílených míst, to se ve skutečnosti v spojitém rozdělení nemůže stát). Definujeme
	$$ T := (W := ) \sum_{i=1}^n r_i·\sgn(X_i) = T^+ - T^-. $$

	Zamítneme nulovou hypotézu, pokud $T$ je moc velké nebo moc malé, tj. $T < Y_{\alpha / 2}$ nebo $T > Y_{1 - \alpha / 2}$ ve správném (TODO?) rozdělení.
\end{definice}

\begin{definice}[Mannův–Whitneyho U-test]
	Máme dvě množiny $X_1, …, X_n$ a $Y_1, …, Y_m$.
	$$ U := \sum_{i=1}^n \sum_{j=1}^m S(X_i, Y_i), \qquad S(X, Y) := \begin{cases}0, & X > Y,\\ \frac{1}{2}, & X = Y,\\ 1, & X < Y.\end{cases} $$

	Nulová hypotéza je $P(X < Y) = P(Y < X)$.
\end{definice}

TODO!!! (Simpson paradox) % 11. přednáška od cca 1:00.

\section{Moment generating function}

\begin{definice}[Moment generating function (MGF)]
	Pokud $X$ je náhodná veličina a $s \in ®R$, potom $M_X(s) := ®E(e^{sX})$.
\end{definice}

\begin{veta}
	$$ M_X(s) = \sum_{k=0}^∞ ®E(X^k) \frac{s^k}{k!}. \qquad (\text{Pro $s$ z intervalu, kde je $M_X(s)$ definováno}.) $$

	\begin{dukazin}
		$$ ®E(e^{s·X}) = ®E\(\sum_{k=0}^∞ \frac{(s·X)^k}{k!}\) = \sum_{k=0}^∞ ®E(X^k) \frac{s^k}{k!}. $$
	\end{dukazin}
\end{veta}

\begin{veta}
	$$ M_{a·X + b} = e^{b·s} M_X(a·s). $$

	\begin{dukazin}
		$$ ®E\(e^{s·(a·X + b)}\) = ®E\(e^{a·s·X}·e^{b·s}\) = e^{b·s} M_X(a·s). $$
	\end{dukazin}
\end{veta}

\begin{veta}
	$X$ a $Y$ nezávislé $\implies$ $M_{X + Y} = M_X·M_Y$.

	\begin{dukazin}
		$$ M_{X + Y}(s) = ®E\(e^{s·(X + Y)}\) = ®E\(e^{s·X}·e^{s·Y}\) = ®E\(e^{s·X}\)·®E\(e^{s·Y}\) = M_X(s)·M_Y(s). $$
	\end{dukazin}
\end{veta}

\begin{veta}
	Pokud $\exists \epsilon > 0\ \forall s \in (-\epsilon, \epsilon): M_X(s) = M_Y(s) \in ®R$, pak $F_X(t) = F_Y(t)$ $\forall t \in ®R$.

	\begin{dukazin}
		Bez důkazu.
	\end{dukazin}
\end{veta}

\begin{veta}
	Pokud $\exists \epsilon > 0\ \forall s \in (-\epsilon, \epsilon): M_{Y_n}(s) \rightarrow M_Z(s) \in ®R$ a $F_Z$ je spojité, pak $F_{Y_n}(t) \rightarrow F_Z(t)$ $\forall t \in ®R$ ($Y_n \overset D\rightarrow Z$).

	\begin{dukazin}
		Bez důkazu.
	\end{dukazin}
\end{veta}

\begin{veta}[Centrální limitní věta]
	$X_1, X_2, …$ nezávislé stejně rozdělené veličiny, $®E X_i = \mu$, $\var X_i = \sigma^2$, potom
	$$ Y_n = \frac{X_1 + … + X_n - n·\mu}{\sigma \sqrt{n}}. $$
	Potom $Y_n \overset D\rightarrow N(0, 1)$.

	\begin{dukazin}
		Použijeme předchozí větu, kde $Z \sim N(0, 1)$, $M_Z = e^{\frac{s^2}{2}}$, zřejmě $F_Z$ je spojitá. Můžeme předpokládat, že $\mu = 0$. Také předpokládejme, že $M_{X_i}(s)$ existuje. Potom $Y_n = \frac{X_1 + … + X_n}{\sigma \sqrt{n}}$. Tedy
		$$ M_{Y_n}(s) = M_{X_1 + … + X_n}\(\frac{s}{\sigma \sqrt{n}}\) = \(M_{X_1}\(\frac{s}{\sigma\sqrt{n}}\)\)^n = \(1 + \sigma^2 \frac{s^2}{2\sigma^2·n} + o\(\frac{s^2}{\sigma^2·n}\)\)^n ≈ $$
		$$ ≈ \(1 + \frac{s^2}{2n}\)^n \rightarrow e^{s^2 / 2} = M_Z(s). $$
		$≈$ je trochu podvod, ale dokáže se jednoduše zlogaritmováním.
	\end{dukazin}
\end{veta}

\begin{veta}[Chernoffova]
	$X_1, …, X_n \sim 1 - 2·Ber\(\frac{1}{2}\)$ jsou nezávislé stejně rozdělené veličiny, $X = X_1 + … + X_n$, $\sigma^2 = \var X = n$, $t > 0$, potom
	$$ P(X ≤ t) = P(X ≥ t) ≤ e^{-\frac{t^2}{2n}}. $$

	\begin{dukazin}
		Pro libovolné $s$ máme
		$$ P(X ≥ t) = P(e^{s·X} ≥ e^{s·t}) ≤ \frac{®E e^{s·X}}{e^{s·t}} = \frac{M_X(s)}{e^{s·t}} = \frac{\(M_{X_1}(s)\)^n}{e^{s·t}} = \frac{\(e^s + e^{-s}\)^n}{2·e^{s·t}} = $$
		$$ = \frac{\(\sum_{k=0}^∞ \frac{s^{2k}}{(2k)!}\)^n}{e^{s·t}} ≤ \frac{\(\sum_{k=0}^∞ \frac{(s^2 / 2)^k}{k!}\)^n}{e^{s·t}} = \frac{e^{n·s^2 / 2}}{e^{s·t}} = e^{\frac{n·s^2}{2} - s·t}. $$
		Následně dosadíme $s = \frac{t}{n}$.
	\end{dukazin}
\end{veta}

TODO(Shannon's coding theorem) % 12. přednáška, cca 1:10 – konec

\end{document}
