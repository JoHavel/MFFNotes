\documentclass[12pt]{article}					% Začátek dokumentu
\usepackage{../../MFFStyle}					    % Import stylu



\begin{document}

% 14. 02. 2022
\section*{Organizační úvod}
\begin{poznamka}
	Podmínkou zápočtu je splnění 1 domácí práce a 1 písemného testu. Není potřeba docházka.

	Bude moodle (přístup dají cvičící). Budou tam poznámky k přednášce, cvičebnice a bude se tam odevzdávat domácí práce.

	Je dobré umět míru.
\end{poznamka}

\section{Úvod}
\begin{poznamka}
	Pravděpodobnost popisuje modely popisující náhodné jevy.

	Statistika se pak snaží popsat reálné věci za pomocí těchto modelů.
\end{poznamka}

\begin{poznamka}[Historie]
	Klasická pravděpodobnost navazuje na dílo Kolmogorova, který popisoval axiomatickou pravděpodobnost.
\end{poznamka}

\section{Pravděpodobnostní prostor}
\begin{definice}[Pravděpodobnostní prostor, pravděpodobnost]
	Pravděpodobnostní prostor je trojice $(\Omega, ©A, P)$, kde $\Omega$ je neprázdná množina, ©A je $\sigma$-algebra a $P$ je pravděpodobnost.

	Pravděpodobnost $P$ je množinová funkce $©A \rightarrow [0, 1]$ splňující:
	\begin{itemize}
		\item $P(A) ≥ 0\ \forall A \in ©A$, (nezápornost)
		\item $P(\Omega) = 1$, (normovanost)
		\item jsou-li $A_i \in ©A$ po dvou disjunktní, pak $P\(\bigcup_{i=1}^∞ A_i\) = \sum_{i=1}^∞ P(A_i)$. ($\sigma$-aditivita)
	\end{itemize}
	
	\begin{poznamkain}[Interpretace]
		$\Omega$ se často nazývá stavový prostor a obsahuje všechny „realizace náhody“ neboli elementární jevy, tj. všechny možnosti, o kterých uvažuji.

		©A je $\sigma$-algebra náhodných jevů. $P$ pak obsahuje veškerou informaci o té dané náhodné situaci.

		Pokud nastal $\omega \in A \in ©A$ ($\omega \in \Omega$), pak nastal jev $A$.
	\end{poznamkain}

	\begin{definicein}[Klasický pravděpodobnostní prostor, diskrétní pravděpodobnostní prostor, spojitý pravděpodobnostní prostor, indikátor]
		$\Omega$ konečná, $©A = 2^\Omega$, $P(\{a\}) = \frac{1}{n}\ \forall a \in \Omega$ je klasický pravděpodobnostní prostor.

		$\Omega$ spočetná (včetně konečná), $©A = 2^\Omega$, $p: \Omega \rightarrow [0, 1]$ je taková, že $p(\omega) ≥ 0\ \forall \omega \in \Omega$ a $\sum_{\omega \in \Omega} = 1$. Položíme $P(A) = \sum_{\omega \in A} p(\omega)\ \forall A \in ©A$ nazýváme diskrétní pravděpodobnostní prostor.

		$\Omega = ®R$, $©A = ©B(®R)$ (resp. $©B_0(®R)$) a $g: ®R \rightarrow [0, ∞)$ měřitelná, že $\int_{®R} g(x) dx = 1$, pak definujeme $P(B) = \int_B g(x) dx$, $b \in ©B(®R) = ©A$ je spojitý pravděpodobnostní prostor. Speciálním případem $g(x) = 1_{[0, 1]}(x)$ je pak tzv. indikátor.
	\end{definicein}
\end{definice}

\begin{definice}[Jev jistý, jev nemožný, podjev, zároveň, alespoň jeden, jev opačný, neslučitelné jevy]
	$\Omega$ je jev jistý, $\O$ je jev nemožný, $A \subset B$ znamená „$A$ je podjev $B$“, $A \cap B$ znamená „nastal $A$ a zároveň $B$“, $A \cup B$ znamená „nastal $A$ nebo $B$“, $A^C$ je jev opačný, $A \cap B = \O$ jsou neslučitelné jevy.
\end{definice}

\begin{veta}
	Buďte $(\Omega, ©A, P)$ pravděpodobnostní prostor a $A, B, A_i \in ©A$ ($i \in ®N$) náhodné jevy. Pak platí:
	\begin{itemize}
		\item $P(\O) = 0$;
		\item $P$ je konečně aditivní;
		\item $P(A^C) = 1 - P(A)$;
		\item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$;
		\item $A \subset B \implies P(A) ≤ P(B)$; (monotonie)
		\item $A_1 \subseteq A_2 \subseteq … \implies P\(\bigcup_{i=1}^∞ A_i\) = \lim_{n \rightarrow ∞} P(A_i)$; (spojitost)
		\item $A_1 \supseteq A_2 \supseteq … \implies P\(\bigcap_{i=1}^∞ A_i\) = \lim_{n \rightarrow ∞} P(A_i)$; (spojitost)
		\item $A_1 \supseteq A_2 \supseteq … \land \bigcap_{i=1}^∞ A_i = \O \implies \lim_{n \rightarrow ∞} P(A_i) = 0$; (spojitost v nule)
		\item $B \subset A \implies P(A \setminus B) = P(A) - P(B)$.
	\end{itemize}

	\begin{dukazin}
		Vše z míry. Pravdědobnost je konečná, předposlední bod vyplývá z předchozího.
	\end{dukazin}
\end{veta}

% 16. 02. 2022

\begin{poznamka}
	28. února bude v 17:20 náhradní přednáška za poslední přednášku.
\end{poznamka}

\begin{veta}[Princip inkluze a exkluze]
	Buď $(\Omega, ©A, P)$ pravděpodobnostní prostor. Pak pro každé $n \in ®N$ a každá $A_i \in ©A$, $i \in ®N$, platí:
	$$ P(\bigcup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i) - \sum_{1 ≤ i ≤ j ≤ n} P(A_i \cap A_j) + … + (-1)^{n-1} P(\bigcap_{i=1}^n A_i). $$

	\begin{dukazin}
		Nebude, v podstatě byl v diskrétce.
	\end{dukazin}
\end{veta}

\section{Podmíněná pravděpodobnost}
\begin{definice}[Podmíněná pravděpodobnost]
	Buďte $A, B \in ©A$ takové, že $P(B) > 0$. Definujeme $P(A|B) = \frac{P(A \cap B)}{P(B)}$ a nazýváme ji podmíněnou pravděpodobností jevu $A$ za podmínky (jevu) $B$.
\end{definice}

\begin{veta}
	Buď $B \in ©A$ takové, že $P(B) > 0$. Pak zobrazení $P(.|B): ©A \rightarrow [0, 1]$ splňuje definici pravděpodobnosti.

	\begin{dukazin}
		Ověříme po bodech: zřejmě $P(A|B) ≥ 0$ $\forall A \in ©A$, $P(\Omega|B) = \frac{P(\Omega \cap B)}{P(B)} = \frac{P(B)}{P(B)} = 1$ a $\sigma$-aditivita plyne ze $\sigma$-aditivity $P(. \cap B)$ a deMorganových pravidel ($B \cap \bigcup_{i=1}^∞ A_i = \bigcup_{i=1}^∞ A_i \cap B$), $P(B)^{-1}$ se prostě z obou stran vytkne.
	\end{dukazin}
\end{veta}

\begin{upozorneni}
	Podmíněná pravděpodobnost nám neříká nic o příčinné souvislosti.
\end{upozorneni}

\begin{pozorovani}[O podmíněné pravděpodobnosti]
	Buďte $A, B, C \in ©A$ a pravděpodobnost „správných“ jevů nenulová. Pak:
	
	\begin{itemize}
		\item $P(A \cup B | C) = P(A|C) + P(B|C) - P(A \cap B|C)$,
		\item $B \subset A \implies P(A|B) = 1$,
		\item $A \cap B = \O \implies P(A|B) = 0$,
		\item $P(A | \Omega) = P(A)$,
		\item pokud $P(\{\omega\}) > 0$, pak $\forall A \in ©A$ platí $P(A|\{\omega\}) = \delta_{\omega}(A)$.
	\end{itemize}

	\begin{dukazin}
		Triviální (buď z definice, nebo z toho, že je to pravděpodobnost).
	\end{dukazin}
\end{pozorovani}

\begin{upozorneni}[Neplatí!]
	$P(A | B \cup C) = P(A|B) + P(A|C)$, ani v případě, že $A \cap B = \O$.
\end{upozorneni}

\begin{veta}[O násobení pravděpodobností]
	Buďte $A_1, A_2, …, A_n \in ©A$ takové, že $P(A_1 \cap A_2 \cap … \cap A_{n-1}) > 0$. Pak
	$$ \hspace{-0.3em}P(A_1 \cap A_2 \cap … \cap A_n) = P(A_n | A_1 \cap … \cap A_{n-1}) · P(A_{n-1} | A_1 \cap A_2 \cap … \cap A_{n-2}) · … · P(A_2 | A_1) · P(A_1).\hspace{-0.3em} $$

	\begin{dukazin}
		Z $P(A_1 \cap … \cap A_{n-1}) > 0$ plyne, že $P(A_1 \cap … \cap A_k) > 0$ pro $k \in [n-1]$, pomocí monotonie pravděpodobnosti. Tedy výraz je dobře definován.

		Dokážeme indukcí: Pro $n = 2$ platí $P(A_1 \cap A_2) = P(A_2|A_1)·P(A_1)$ z definice. Z $n-1$ na $n$: ($B := A_1 \cap … \cap A_{n-1}$)
		$$ P(A_1 \cap A_2 \cap … \cap A_n) = P(B \cap A_n) \overset{\text{def}}= P(A|B)·P(B) \overset{\text{IP}}= $$
		$$ = P(A_n | A_1 \cap … \cap A_{n-1}) · P(A_{n-1} | A_1 \cap A_2 \cap … \cap A_{n-2}) · … · P(A_2 | A_1) · P(A_1). $$
	\end{dukazin}
\end{veta}

\begin{veta}[O celkové pravděpodobnosti]
	Buďte $A, B_1, B_2, …$ náhodné jevy takové, že $P(\bigcup_n B_n) = 1$ a $B_i \cap B_j = \O$ $\forall i ≠ j$ a $P(B_i) > 0\ \forall i$. Potom $P(A) = \sum_n P(A | B_n)·P(B_n)$.
	
	\begin{dukazin}
		Víme $P\(\(\bigcup_n B_n\)^c\) = 0$, a tedy $P(A) = P\(A \cap \bigcup_n B_n\) + P\(A \cap \(\bigcup_n B_n\)\) = P\(A \cap \bigcup_n B_n\)$, protože $P$ je konečně-aditivní a platí monotonie. Dle de Morganových pravidel (a toho, že průnik s další množinou zachovává disjunktnost):
		$$ P(A) = P\(\bigcup_n(A \cap B_n)\) = \sum_n P(A \cap B_n) = \sum_n P(A|B_n) · P(B_n). $$
	\end{dukazin}
\end{veta}

\begin{veta}[Bayesova]
	Za předpokladů věty o celkové pravděpodobnosti a $P(A) > 0$, platí $P(B_i|A) = \frac{P(A|B_i)P(B_i)}{\sum_n P(A|B_n)P(B_n)}$.

	\begin{dukazin}
		Snadný z definice podmíněné pravděpodobnosti a věty o celkové pravděpodobnosti.
	\end{dukazin}
\end{veta}

\begin{priklad}[Pólyovo urnové schéma]
	Máme v urně $n$ koulí $k$ různých barev. Náhodně taháme z urny. Po vytažení koule do urny vytaženou kouli vrátíme a s ní i $\Delta$ (pevný parametr) koulí stejné barvy.

	Podle volby $\Delta$ máme 2 základní schémata: $\Delta = -1$ (tahání bez vracení) a $\Delta = 0$ (tahání s vracením).
\end{priklad}

% 21. 02. 2022

\begin{definice}[Nezávislé jevy]
	Náhodné jevy $A$ a $B$ jsou nezávislé, pokud platí $P(A \cap B) = P(A)·P(B)$.
\end{definice}

\begin{upozorneni}
	Zase to nemá nic do činění s kauzalitou.
\end{upozorneni}

\begin{veta}
	Jsou-li dva jevy $A$ a $B$ nezávislé, pak jsou i jevy $A$ a $B^c$ nezávislé.

	Je-li navíc $P(B) > 0$, pak $P(A|B) = P(A)$.

	\begin{dukazin}
		$$ P(A \cap B^c) = P(A) - P(A \cap B) = P(A) - P(A)·P(B) = P(A)·(1 - P(B)) = P(A)·P(B). $$

		$$ P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A)·P(B)}{P(B)} = P(A). $$
	\end{dukazin}
\end{veta}

\begin{definice}[Vzájemná nezávislost]
	Buď $\{A_\lambda\}_{\lambda \in \Lambda}$ systém náhodných jevů. Pak říkáme, že tyto jevy jsou (vzájemně) nezávislé, pokud pro každou konečnou množinu $I \subset \Lambda$ (dále $I \in ©F(\Lambda)$) platí $P(\bigcap_{i \in I} A_i) = \prod_{i \in I} P(A_i)$.
\end{definice}

\begin{veta}
	Buď $C = \{B_1, …, B_k\}$, $k \in ®N$, systém nezávislých jevů. Nahradíme-li libovolnou podmnožinu těchto jevů jejich doplňky, dostaneme opět systém nezávislých jevů

	\begin{dukazin}
		Indukcí podle velikosti nahrazované množiny. (Použije se předchozí věta.)
	\end{dukazin}
\end{veta}

\begin{veta}
	Jsou-li jevy $A_1, …, A_n, B_1, …, B_m$ vzájemně nezávislé a $P(B_1 \cap … \cap B_m) > 0$, pak
	$$ P(A_1 \cap … \cap A_n|B_1 \cap … \cap B_m) = P(A_1 \cap … \cap A_n) = P(A_1)·…·P(A_n). $$

	\begin{dukazin}
		Snadný.
	\end{dukazin}
\end{veta}

\section{Náhodné veličiny}
\begin{definice}[Náhodný element]
	Buďte $(\Omega, ©A)$ a $(\Omega', ©A')$ stavové prostory. Pak každé měřitelné zobrazení $X: \Omega \rightarrow \Omega'$ nazveme náhodný element z $\Omega'$.
\end{definice}

\begin{definice}[Náhodná veličina]
	Měřitelné zobrazení $X: (\Omega, ©A) \rightarrow (®R, ©B(®R))$ nazveme (reálnou) náhodnou veličinou.
\end{definice}

\begin{definice}[Značení]
	Místo $\{\omega \in \Omega | X(\omega) ≤ a\}$ píšeme $\{X ≤ a\}$, místo $P(\{X ≤ a\})$ píšeme $P(X ≤ a)$.
\end{definice}

\begin{definice}
	Buď $X$ náhodná veličina. $X^{-1}(©B(®R))$ značíme $\sigma(X)$ a nazýváme $\sigma$-algebrou náhodných jevů generovaných náhodnou veličinou $X$ ($\sigma$-algebra indukovaná $X$).
\end{definice}

% 23. 02. 2022

\begin{definice}[Rozdělení náhodné veličiny]
	Rozdělením náhodné veličiny $X: (\Omega, ©A) \rightarrow (®R, ©B)$ rozumíme indukovanou pravděpodobnostní míru $P_X$ na $(®R, ©B)$ definovanou jako
	$$ P_X(B) = P(\{\omega \in \Omega | X(\omega) \in B\}) = P(X^{-1}(B)), \qquad B \in ©B. $$

	\begin{poznamkain}[A důkaz, že je to pravděpodobnostní míra]
		$P_X$ je obraz míry $P$ v zobrazení $X$.
	\end{poznamkain}
\end{definice}

\begin{veta}[O přenosu integrace pro $P_X$]
	Buď $X$ náhodná veličina a buď $h$ měřitelná funkce $(®R, ©B) \rightarrow (®R, ©B)$. Pak platgí
	$$ \int_{\Omega} h(X(\omega)) d P(\omega) = \int_{®R} h(x) d P_X(x), $$
	pokud existuje alespoň jedna strana.

	\begin{dukazin}
		Speciální případ věty o obrazu míry z TMI1.
	\end{dukazin}
\end{veta}

\begin{definice}[Hustota náhodné veličiny]
	Buď $X$ náhodná veličina, $P_X$ její rozdělení a $\mu$ $\sigma$-konečná míra na $(®R, ©B)$ taková, že $P_X \ll \mu$. Potom $f(x) = \frac{d P_X}{d\mu}(x)$ se nazývá hustota náhodné veličiny $X$ vzhledem k míře $\mu$.

	\begin{poznamkain}
		$f(x)$ je určena jednoznačně $\mu$-skoro všude. Pokud pro $g: (®R, ©B) \rightarrow (®R, ©B)$ měřitelnou platí
		$$ \int_{®R} |g(x)| d P_X(x) < ∞ \qquad (\lor g(x) ≥ 0 \forall x), $$
		pak $\int_{®R} g(x) d P_X(x) = \int_{®R} g(x) f(x) d\mu(x)$.
	\end{poznamkain}
\end{definice}

\begin{veta}
	Buď $X$ náhodná veličina, pak platí následující rovnosti:
	$$ P(X \in B) := P(\{\omega \in \Omega | X(\omega) \in B\}) = \int_{\Omega} 1_B(X(\omega)) d P(\omega) = \int_{®R} 1_B(x) d P_X(x) = \int_{B} d P_X(x) = P_X(B) = $$
	$$ = \int_{®R} 1_B(x) d P_X(x) = \int_{®R} 1_B(x) f(x) d\mu(x) = \int_B f(x) d\mu(x). $$
	
	\begin{dukazin}
		Je to jen sesypání faktů, které už známe.
	\end{dukazin}
\end{veta}

\begin{definice}[Distribuční funkce]
	Mějme náhodnou veličinu $X: (\Omega, ©A) \rightarrow (®R, ©B)$. Funkci $F_X: ®R \rightarrow [0, 1]$ definovanou jako $F_X(x) = P(X ≤ x)$ nazveme distribuční funkce náhodné veličiny $X$.

	\begin{poznamkain}
		Definice se shoduje s distribuční funkcí z TMI1. (A tedy platí věta o vlastnostech distribuční funkce, s tím, že dokonce $\lim_{x \rightarrow +\infty} F_X(x) = 1$.)
	\end{poznamkain}
\end{definice}

\begin{veta}
	Buď $F: ®R \rightarrow ®R$ splňující vlastnosti distribuční funkce a $\lim_{x \rightarrow +∞} F_X(x) = 1$. Pak existuje pravděpodobnostní prostor $(\Omega, ©A, P)$ a náhodná veličina $X: (\Omega, ©A) \rightarrow (®R, ©B)$ taková, že $F_X = F$.

	\begin{dukazin}
		Z TMI1 víme, že existuje Lebesgueova-Stieltjesova míra $\mu$, jejíž distribuční funkce je $F$. Tj. $\mu(\(-∞, a\]) = F(a)$. Teď chybí jen dodefinovat $(\Omega, ©A, P)$ a $X$. Položíme $(\Omega, ©A, P) = (®R, ©B, \mu)$ a $X = \id_{®R}$.
	\end{dukazin}
\end{veta}

% 28. 02. 2022

\begin{definice}[Názvosloví: diskrétní náhodná veličina, absolutně spojitá veličina]
	Mějme diskrétní náhodnou veličinu $P_X ≡ \mu_d$, tj. existuje nejvýše spočetná $\{x_i\}_{i \in I} \subset ®R$ a $\{p_i\}_{i \in I} \subset \(0, 1\]$ takových, že $\sum_{i \in I} p_i = 1$ a platí $P_X = \sum_{i \in I} p_i \delta_{x_i}$.

	Potom nutně $F_X(x) = \sum_{i \in I} p_i 1_{[x, ∞)}(x)$ a také platí, že $P_X \ll \nu$, kde $\nu$ je čítací míra na $\{x_i\}_{i \in I}$.

	(Absolutně) spojitá náhodná veličina je taková, že $P_X = \mu_a \ll \lambda$, takže $P_X(B) \int_B f(x) d\mu$.
\end{definice}

\begin{definice}[Kvantilová funkce]
	Buď $F_X$ distribuční funkce náhodné veličiny $X$. Funkce $F^{-1}_X (u) = \int\{x | F_X(x) ≥ u\}$, $u \in (0, 1)$ se nazývá kvantilová funkce náhodné veličiny $X$.

	\begin{poznamkain}
		Bude potřeba později. Teď jen: Je neklesající a zleva spojitá. Lze z ní jednoznačně odvodit $F_X$.
	\end{poznamkain}
\end{definice}

\begin{upozorneni}
	Kvantilová funkce obecně není inverzní funkcí k $F_X$, protože inverzní funkce nemusí existovat. Ale pro $F_X$ rostoucí a spojitou je $F_X^{-1}$ inverzní funkcí k $F_X$.
\end{upozorneni}

\subsection{Střední hodnota, rozptyl a momenty náhodné veličiny}
\begin{definice}[Střední hodnota]
	Střední hodnota náhodné veličiny $X$ je číslo $®E X$ dané výrazem $®E X = \int_{\Omega} X(\omega) d P(\omega)$, pokud má integrál smysl.
\end{definice}

\begin{definice}[Medián]
	Medián rozdělení náhodné veličiny $X$ je číslo $q_{\frac{1}{2}}$ splňující $P(X ≤ q_{\frac{1}{2}}) ≥ \frac{1}{2}$ a $P(X ≥ q_{\frac{1}{2}}) ≥ \frac{1}{2}$.
\end{definice}

% 28. 02. 2022

\begin{veta}
	Buď $X$ náhodná veličina a $g: ®R \rightarrow ®R$ měřitelná funkce. Pak $g(X)$ je také náhodná veličina a $®E g(X) = \int_{®R} g(x) d P_X(x) = \int_{®R} g(x) dF_X(x)$, pokud alespoň jeden z výrazů existuje.

	\begin{dukazin}
		Složení 2 měřitelných funkcí je měřitelné, tj. $g(X)$ je opravdu náhodná veličina.
		$$ ®E g(X) = \int_{\Omega} g(X(\omega)) = d P(\omega) = \int_{®R} g(x) d P_X(x). $$
		Druhá rovnost plyne ze vztahu mezi $P_X$ a její distribuční funkcí.
	\end{dukazin}
\end{veta}

\begin{veta}[Základní vlastnosti $®E X$]
	Buďte $X, Y$ náhodné veličiny na stejném pravděpodobnostním prostoru $(\Omega, ©A, P)$. Pak platí
	$$ ®E(a + bX) = a + b ®E X,\qquad X \in L^1, a, b \in ®R, $$
	$$ ®E(X + Y) = ®E X + ®E Y, \qquad X, Y \in L^1, $$
	$$ P(X ≥ 0)  = 1 \implies ®E X ≥ 0, \qquad (\text{obecněji } P(X \in [a, b]) = 1 \implies ®E X \in [a, b]), $$
	$$ X \in L^1 \implies |X| \in L^1, $$
	$$ X ≤ Y, P\text{-skoro všude } \implies ®E X ≤ ®E Y (\text{pokud existují}). $$

	\begin{dukazin}
		Snadný, aplikace míry.
	\end{dukazin}
\end{veta}

\begin{definice}[Názvosloví: $P$-skoro jistě]
	$P$-skoro jistě znamená $P$-skoro všude.
\end{definice}

\begin{definice}[$n-tý$ moment]
	$n$-tý moment náhodné veličiny $X$ definujeme jako $®E X^n$, $n \in ®N$.

	$n$-tý absolutní moment náhodné veličiny $X$ definujeme jako $®E |X|^n$, $n \in ®N$.
	
	$n$-tý centrální moment náhodné veličiny $X$ definujeme jako $®E (X - ®E X)^n$, $n \in ®N$, pokud $®E X \in ®R$.
	
	$n$-tý absolutní centrální moment náhodné veličiny $X$ definujeme jako $®E |X - ®E X|^n$, $n \in ®N$, pokud $®E X \in ®R$.

	\begin{poznamkain}
		1-ní moment je $®E X$. První centrální moment je 0.
	\end{poznamkain}
\end{definice}

\begin{definice}[Rozptyl]
	Rozptyl náhodné veličiny $X$ je definován jako $®E(X - ®E X)^2$. Značí se $\var X$.

	\begin{poznamkain}
		Rozptyl je střední čtvercová odchylka $X$ od $®E X$. $\var X = ®E(X - ®E X)^2 ≥ 0$. $\var X = 0$ právě tehdy, když $X = ®E X$ skoro jistě.
	\end{poznamkain}
\end{definice}

\begin{veta}[Základní vlastnosti rozptylu]
	$$ \var(a + b X) = b^2 \var X, \qquad a, b \in ®R \land X \in L^2. $$

	\begin{dukazin}
		$$ \var(a + b X) = ®E(a + bX - ®E(a + b X))^2 = ®E(a + bX - a - b ®E^X) = ®E(b X - b ®E X) = ®E(b(X - ®E X))^2 = b^2 ®E(X - ®E X)^2 = b^2 \var X. $$
	\end{dukazin}
\end{veta}

\begin{veta}[Čebyševova nerovnost]
	Buď $X \in L^1$ náhodná veličina. Pak $P(|X - ®E X| ≥ a) ≤ \frac{\var X}{a^2}$, $\forall a > 0$.

	\begin{dukazin}
		Viz TMI1.
	\end{dukazin}
\end{veta}

\begin{definice}[Markovova nerovnost]
	Buď $X \in L^n$, $n \in ®N$, náhodná veličina. Pak $P(|X| ≥ a) ≤ \frac{®E |X|^n}{a^n}$, $\forall a > 0$.

	\begin{dukazin}
		Obdobně Čebyševově větě.
	\end{dukazin}
\end{definice}

\begin{veta}[Nerovnost mezi $L^p$ normami na pravděpodobnostních prostorech]
	Buď $X$ náhodná veličina, $0 < \alpha < \beta \in ®R$ a $®E |X|^\beta < ∞$. Pak platí $\sqrt[\alpha]{®E |X|^\alpha} ≤ \sqrt[\beta]{®E |X|^\beta}$, a speciálně tedy platí $®E |X| ≤ \sqrt{®E X^2}$.

	\begin{dukazin}
		$$ ®E |X|^\alpha = \int_{®R} |x|^\alpha d P_X(x) = \int_{®E} |x|^\alpha·1 d P_X(x) \overset{\text{Hölder na $p = \frac{\beta}{\alpha}$}} ≤ \(\int_{®R} |x|^\beta d P_X(x)\)^{\frac{\alpha}{\beta}}·\(\int_{®R} 1^q dP_X(x)\)^{\frac{1}{q}} = … · 1. $$
		(Integrál napravo je konečný z předpokladů této věty, tedy splňujeme předpoklady Höldera.) Odmocněním $\alpha$ dostáváme přesně chtěnou nerovnost.
	\end{dukazin}
\end{veta}

% 02. 03. 2022

\begin{priklady}[Absolutně spojitá rozdělení]
	Rovnoměrné rozdělení intervalu $[a, b]$, $a < b \in ®R$ značíme $R([a, b])$ a jeho hustota je až na konstantu Lebesgueova míra: $f_X(x) = \frac{1}{b - a}1_{(a, b)}(x)$.
	$$ F_X(t) = \begin{cases}0, & t ≤ a, \\ \frac{x - a}{b - a}, & t \in [a, b], \\ 1, & t ≥ b.\end{cases} \qquad ®E X = \int_a^b x · \frac{1}{b - a} d x = \frac{b + a}{2}, \var X = ®E X^2 - (®E X)^2 = \frac{(b - a)^2}{12}. $$

	Exponenciální rozdělení s parametrem $\lambda > 0$ značíme $Exp(\lambda)$. $P(X > t) = e^{-t \lambda}$, $t > 0$. $F_X(t) = P(X ≤ t) = 1 - e^{-\lambda t}$ pro $t ≥ 0$ a $0$ pro $t ≤ 0$. $f_X(x) = \lambda e^{-\lambda x}$, $x ≥ 0$ a $f_X(x) = 0$ jinak. $®E X = \frac{1}{\lambda}$, $\var X = \frac{1}{\lambda^2}$.

	\begin{poznamkain}
		Exponenciální rozdělení má vlastnost ztráty paměti, tedy že $P(X > s + t | X > s) = P(X > t)$, $s, t > 0$.
	\end{poznamkain}

	Normální (Gaussovo) rozdělení: Normované $N(0, 1)$ je $f_X(x) = \frac{1}{\sqrt{2 \pi}}e^{-\frac{x^2}{2}}$, $x \in ®R$. $F_X(t) = \int_{-∞}^t \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}} dx$. Tyto $F_X$ a $f_X$ se často značí $\Phi$ a $\phi$. $®E X = 0$ ($x·\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ je lichá funkce), $\var X = ®E(X - ®E X)^2 = ®E X^2 = 1$. $®E X^{2k + 1} = 0$.

	Obecné $N(\mu, \sigma^2)$, $\mu \in ®R$, $\sigma^2 > 0$ má $f_X(x) = \frac{1}{\sqrt{2 \pi \sigma^2}}·e^{-\frac{(x - \mu)^2}{2 \sigma^2}}$, $x \in ®R$.
\end{priklady}

\begin{tvrzeni}
	Buď $X$ nezáporná (tj. $P(X ≥ 0) = 1$) absolutně spojitá náhodná veličina, která splňuje $P(X > s + t | X > s) = P(X > t)$, $\forall s, t > 0$, pak $X \sim Exp$.

	\begin{dukazin}
		Dělat nebudeme.
	\end{dukazin}
\end{tvrzeni}

\begin{veta}
	$X \sim N(0, 1)$ a $Y := \sigma X + \mu$, pro $\sigma > 0$, $\mu \in ®R$. Pak $Y \sim N(\mu, \sigma^2)$.

	\begin{dukazin}
		TODO!!!
	\end{dukazin}
\end{veta}

\begin{dusledek}
	$$ F_Y(t) = P(Y ≤ t) = P(\sigma Z + x ≤ t) = P\(Z ≤ \frac{t - \mu}{\sigma}\) = \Phi\(\frac{t - \mu}{\sigma}\). $$
\end{dusledek}

\begin{dusledek}
	$$ ®E Y = ®E(\sigma Z + \mu) = \mu + \sigma ®E Z = \mu + 0 = \mu. $$
	$$ \var Y = \var(\sigma Z + \mu) = \sigma^2·\var Z = \sigma^2. $$
\end{dusledek}

\begin{veta}[Rozdělení funkce náhodné veličiny]
	Buď $X$ náhodná veličina s distribuční funkcí $F_X$, $g: ®R \rightarrow ®R$ měřitelná funkce. Pak $Y = g(X)$ je náhodná veličina s distribuční funkcí $F_Y(y) = \int_{\{x|g(x) ≤ Y\}} d F_X(x)$.

	\begin{dukazin}
		$$ F_Y(y) = P(Y ≤ y) = P(g(X) ≤ Y) = P_X(\{x | g(x) ≤ y\}) = \int_{\{x | g(x) ≤ y\}} d F_X(x). $$
	\end{dukazin}
\end{veta}

% 07. 03. 2022

\section{Náhodné vektory}
\begin{definice}[Náhodný vektor]
	Měřitelné zobrazení $¦X: (\Omega, ©A, P) \rightarrow (®R^n, ©B^n)$, $n \in ®N$, nazveme náhodným vektorem.
\end{definice}

\begin{definice}[Rozdělení náhodného vektoru]
	Rozdělením náhodného vektoru $¦X: (\Omega, ©A, P) \rightarrow (®R^n, ©B^n)$ nazveme indukovanou pravděpodobnostní míru $P_{¦X}$ na $(®R^n, ©B^n)$ definovanou jako $P_{©X}(B) = P(\{\omega \in \Omega | ©X(\omega) \in B\})$, $B \in ©B^n$.
\end{definice}

\begin{definice}[(Sdružená) distribuční funkce]
	(Sdružená) distribuční funkce náhodného vektoru ¦X je definována jako
	$$ F_{¦X} (¦x) = P(\bigcup_{i=1}^b (X_i ≤ x_i)), \qquad \forall ¦x = (x_1, …, x_n) \in ®R^n. $$

	\begin{poznamkain}
		$F_{¦X}$ jednoznačně určuje $P_{¦X}$.
	\end{poznamkain}
\end{definice}

\begin{veta}[O marginální distribuční funkci]
	Buď ¦X $n$-rozměrný náhodný vektor s distribuční funkcí $F_{¦X}$. Pak pro každé $¦x \in ®R^n$ platí
	$$ \lim_{x_n \rightarrow ∞} F_{¦x}(x_1, …, x_n) = F_{(X_1, …, X_{n-1})^T} (x_1, …, x_{n-1}), $$
	kde $F_{(X_1, …, X_{n-1})^T}$ je distribuční funkce náhodného vektoru $(X_1, …, X_{n-1})^T$.

	\begin{dukazin}
		Použijeme Heineho větu: Nechť máme posloupnost čísel $\{y_k\}_{k=1}^∞$ takových, že $y_k \rightarrow ∞$. Označme $B = \bigcup_{i=1}^{n-1} \{X_i ≤ x_i\}$. $B_k = \(\bigcup_{i=1}^{n-1} \{X_i ≤ x_i\}\) \cap \{X_n ≤ y_k\}$, $D_k = \(\bigcup_{l=k}^∞ B_l^c\)^c$, $k \in ®N$. Zřejmě $D_k \subseteq B_k \subset B = \bigcup_{k=1}^∞ B_k$ a $D_k \nearrow B$. Ze spojitosti $P$ máme $\lim_{k \rightarrow ∞} P(D_k) = P(B)$. Nakonec z monotonie $P$ máme $P(D_k) ≤ P(B_k) ≤ P(B)$, tedy ze dvou strážníků $\lim_{k\rightarrow ∞} P(B_k) = P(B)$.
	\end{dukazin}
\end{veta}

\begin{poznamka}
	Pro každou permutaci $\pi \in ©S_n$ platí
	$$ F_{¦X}(x_1, …, x_n) = F_{(X_{\pi(1)}, …, X_{\pi(n)})^T}(x_{\pi(1)}, …, x_{\pi(n)}). $$

	Rozdělení $P_{¦Y}$ podvektoru $¦Y = (X_j)_{j \in J}$, $J \subset \{1, …, n\} = I$ se nazývá marginální rozdělení (a distribuční funkce se nazývá marginální distribuční funkce).

	Rozdělení $(X_1, …, X_n)$ určuje rozdělení $X_1, …, X_n$, ale ne naopak.
\end{poznamka}

\begin{definice}[Značení]
	Mějme dva body $¦a, ¦b \in ®R^n$ a buď $¦c \in ®R^n$ takový, že $c_i \in \{a_i, b_i\}, \forall i \in [n]$. Potom
	$$ \Delta_{n, k} = \{c | c_i = a_i \text{ právě pro $k$ indexů}\}, \qquad k \in [n]_0. $$
\end{definice}

\begin{veta}[O vlastnostech sdružené distribuční funkce]
	Distribuční funkce náhodného vektoru ¦X splňuje:

	\begin{enumerate}
		\item $\lim_{x_i \rightarrow ∞ \forall i} F_{¦X} (¦x) = 1$;
		\item $\forall j \forall x_1, …, x_{j-1}, x_{j+1}, …, x_n: \lim_{x_j \rightarrow -∞} F_{¦X}(¦x) = 0$;
		\item $F_{¦X}$ je zprava spojitá v každé proměnné;
		\item $\forall ¦a, ¦b \in ®R^n$, $a_i < b_i$, $\forall i \in [n]$ platí $\sum_{k=0}^n (-1)^k \sum_{¦c \in \Delta_{n, k}} F_{¦X}(¦c) ≥ 0$. („Monotonie“.)
	\end{enumerate}

	\begin{dukazin}
		1. Uvědomme si, že $x_i \rightarrow ∞, \forall i \in [n] \Leftrightarrow \min_{i \in [n]} x_i \rightarrow ∞$. Z monotonie pravděpodobnosti $P$ máme $1 ≥ F_{¦X}(¦x) ≥ F_¦X (\min_i x_i·(1, …, 1))$. Stačí ukázat, že pro funkci $H(x) := F_{¦X}(x·(1, …, 1))$ platí $\lim_{x \rightarrow +∞} H(x) = 1$. $H(x)$ je neklesající funkce $x$ (z monotonie pravděpodobnosti $P$ a definice distribuční funkce) a $H(x) ≤ 1, \forall x \in ®R$. Takže musí $\exists \lim_{x \rightarrow ∞} ≤ 1$ a nutně bude i rovna limitě $\lim_{®N \ni k \rightarrow ∞}(k)$. Označme $B_k = (-∞, k·(1, …, 1)]$. Platí $B_k \nearrow ®R^n$, takže ze spojitosti pravděpodobnosti $H(k) = P_{¦X}(B_k) \rightarrow 1$.

		2. 3. analogicky (za domácí úkol).

% 09. 03. 2022

		4. (jen pro $n = 2$, pro $n > 2$ je důkaz zbytečně technický):
		$$ \sum_{k=0}^n (-1)^k \sum_{¦c \in \Delta_{n, k}} F_{¦X}(¦c) = F_{¦X}(b_1, b_2) - \[F_{¦X}(b_1, a_2) + F_{¦x}(a_1, b_2)\] + F_{¦X}(a_1, a_2) = $$
		$$ = \[F_{¦X}(b_1, b_2) - F_{¦X}(b_1, a_2)\] - \[F_{¦X}(a_1, b_2) - F_{¦x}(a_1, a_2)\] = $$
		$$ = P(X_1 ≤ b_1 \land a_2 < X_2 ≤ b_2) - P(X_1 ≤ a_1 \land a_2 < X_2 ≤ b_2) = $$
		$$ = P(a_1 < X_1 ≤ b_1 \land a_2 < X_2 ≤ b_2). $$
	\end{dukazin}
\end{veta}

\begin{poznamka}
	Pro každý interval $(¦a, ¦b]$ v $¦R^n$ definujeme $\mu_F((¦a, ¦b]) = 4.$ z předchozího pro $F$ splňující tvrzení předchozí věty. Potom lze rozšířit $\mu_F$ na konečnou borelovskou míru na $®R^n$ a té se říká Lebesgueova-Stieltjesova míra příslušná $F$.

	Pokud $F = F_{¦X}$ od ¦X s rozdělením $P_{¦X}$, pak nutně $\mu_F = P_{¦X}$, neboť se rovnají na intervalech $\{(¦a, ¦b]| ¦a < ¦b \in ®R^n\}$, což je systém uzavřený na konečné průniky generující $©B^n$.
\end{poznamka}

\begin{veta}
	Nechť $F$ splňuje vlastnosti sdružené distribuční funkce. Pak $\exists$ pravděpodobnostní prostor $(\Omega, ©A, P)$ a náhodný vektor ¦X takový, že $F = F_{¦X}$.

	\begin{dukazin}
		Vezměme $\Omega := ®R^n$, $©A = ©B^n$, $P = \mu_F$ a $¦X = \id$. Pak je ¦X zřejmě měřitelné, tedy je to náhodný vektor. Navíc
		$$ F_{¦X}(¦x) = P(\{\omega \in \Omega | \omega_i ≤ x_i, \forall i \in [n]\}) = \mu_F((-∞, ¦x]) = F(¦x). $$
	\end{dukazin}
\end{veta}

\begin{definice}[Diskrétní rozdělení]
	Náhodný vektor ¦X má diskrétní rozdělení, pokud $\exists$ (konečná nebo spočetná) množina $\{¦x_i\}_{i \in I} \subset ®R^n$ a hodnoty $\{p_i\}_{i \in I}$ splňující $\forall i \in I: p_i \in (0, 1]$, $\sum_{i \in I} p_i = 1$, tak že $P(¦X = ¦x_i) = p_i$.

	\begin{poznamkain}
		Pak nutně $P_{¦X} = \sum_{i \in I} p_i \delta_{¦x_i}$.

		Také zřejmě marginální rozdělení jsou též diskrétní.
	\end{poznamkain}
\end{definice}

\begin{definice}[(Absolutně) spojité rozdělení]
	Náhodný vektor ¦X má (absolutně) spojité rozdělení, existuje-li nezáporná měřitelná funkce $f_{¦X}: ®R^n \rightarrow ®R$ taková, že
	$$ F_{¦X}(¦x) = \int_{(-∞, x_1]} \int_{(-∞, x_2]} … \int_{(-∞, x_n]} f_{¦x}(t_1, …, t_n) dt_n … dt_1, \qquad \forall ¦x \in ®R^n. $$

	\begin{poznamka}
		To nastává právě tehdy, když $P_{¦x} \ll \lambda^n$. Potom $f_{¦x} = \frac{\partial^n}{\partial x_1 … \partial x_n} F_{¦x}$ $\lambda^n$ skoro všude.
	\end{poznamka}
\end{definice}

\begin{veta}[O hustotě $P_{¦X}$ vzhledem k součinové referenční míře]
	Buď $P_{¦X}$ rozdělení $n$-rozměrného náhodného vektoru ¦X. Nechť $P_{¦X} \ll \nu_1 \otimes … \otimes \nu_n$ (součin $\sigma$-konečných měr na ®R). Pak $P_{X_i} \ll \nu_i$, $\forall i \in [n]$, a existují nezáporné měřitelné funkce $f_{¦x}: ®R^n \rightarrow [0, ∞)$, $f_{x_i}: ®R \rightarrow [0, ∞)$, $i \in [n]$ takové, že
	$$ P_{¦X}((-∞, ¦x]) = F_{¦X}(¦x) = \int_{(-∞, x_1]} … \int_{(-∞, x_n]} f_{¦x}(t_1, …, t_n) d\nu_n(t_n) … d\nu_1(t_1), \qquad \forall ¦x \in ®R^n, $$
	$$ P_{¦X}(B) = \int_B f_{¦x}(¦t) d(\nu_1 \otimes … \otimes \nu_n)(¦t), \qquad \forall B \in ©B^n. $$
	Navíc pro každé $x_i \in ®R$, $i \in [n]$ platí
	$$ F_{X_i}(x_i) = \int_{(-∞, x_i]} f_{x_i}(t) d\nu_i(t), $$
	kde $f_{X_i}(y_i) = \int_{®R^{n-1}} f_{¦x}(y_1, …, y_n) d(\nu_1 \otimes … \otimes \nu_{i-1} \otimes \nu_{i+1} \otimes … \otimes \nu_n) (y_1, …, y_{i-1}, y_{i+1}, …, y_n)$ pro $\nu_i$-skoro všechna $y_i \in ®R$.

	\begin{dukazin}
		Snadný: existence a tvrzení o $f_{¦X}$ plyne z Radon-Nikodymovy věty a přepis v 1. vzorci a tvrzení o $f_i$ plyne z Fubiniovy věty a věty o marginální distribuční funkci.
	\end{dukazin}
\end{veta}

\section{Nezávislé náhodné veličiny}
\begin{definice}[(Vzájemně) nezávislé náhodné veličiny]
	Buď $\{X_i\}_{i \in I}$ systém náhodných veličin na $(\Omega, ©A, P)$, kde $I ≠ \O$ je libovolná indexová množina. $\{X_i\}_{i \in I}$ nazveme (vzájemně) nezávislé, pokud $\forall$ konečnou neprázdnou $J \subset I$, platí
	$$ P(\bigcap_{i \in J} \{X_i \in B_i\}) = \prod_{i \in J} P(X_i \in B_i), \qquad B_i \in ©B, i \in J. $$

	\begin{poznamka}
		Pro nezávislé náhodné veličiny jsou jevy $(X_i \in B_i)$ nezávislé.
	\end{poznamka}
\end{definice}

\begin{veta}[O rozdělení vektoru s nezávislými složkami]
	Buď $¦X = (X_1, …, X_n)^T$ náhodný vektro. Pak $\{X_i\}_{i=1}^n$ jsou nezávislé náhodné veličiny právě tehdy, když $P_{¦X} = P_{X_1} \otimes … \otimes P_{X_n}$.

	\begin{dukazin}
		„$\impliedby$“ zřejmě. „$\implies$“: $\forall B_1, …, B_n \in ©B$ platí $P_{¦X}(B_1 \times … \times B_n) = \prod_{i=1}^n P_{x_i}(B)$ plyne z definice nezávislosti, takže $P_{¦X}$ se rovná součinové míře na měřitelných obdélnících $\{B_1 \times … \times B_n | B_i \in ©B\}$, ale tento systém je uzavřený na konečné průniky a generuje $©B^n$, tedy z věty o jednoznačnosti míry se $P_{¦X}$ a $P_{X_1} \otimes … \otimes P_{X_n}$ rovnají na celém $©B^n$.
	\end{dukazin}
\end{veta}

\begin{veta}[O distribuční funkci náhodného vektoru s nezávislými složkami]
	Za předpokladů předchozí věty platí, že $\{X_i\}_{i=1}^n$ jsou vzájemně nezávislé právě tehdy, když $F_{¦X}(¦x) = \prod_{i=1}^n F_{X_i}(x_i)$, $\forall ¦x \in ®R^n$.

	\begin{dukazin}
		„$\implies$“ zřejmě neboť $F_{¦X}(x_1, …, x_n) = P(\bigcup …) = \prod P(X_i \in (-∞, x_i]) = \prod_{i=1}^n F_{X_i}(x_i)$. „$\impliedby$“: Množiny $(-∞, …] \times …$ tvoří systém uzavřený na konečné průniky a generující $©B^n$, tedy z rovnosti $P_{¦X}$ a součinové míry na tomto systému už plyne rovnost dvou měr na $©B^n$ (pomocí věty o jednoznačnosti míry).
	\end{dukazin}
\end{veta}

% 14. 03. 2022

\begin{veta}[O hustotě vektoru s nezávislými složkami]
	Buď $P_{¦X}$ rozdělení $n$-rozměrného náhodného vektoru ¦X splňující $P_{¦X} \ll \nu_1\otimes … \otimes \nu_n$ (součin $\sigma$-konečných měr) a buď $f_{¦X}$ hustota náhodného vektoru ¦X. Pak náhodné veličiny $X_1$, …, $X_n$ jsou vzájemně nezávislé $\Leftrightarrow$ $f_{¦X} (x_1, …, x_n) = \prod_{i=1}^n f_{X_i}(x_i)$ pro $\nu_1 \otimes … \otimes \nu_n$-skoro všechna $¦x \in ®R^n$, kde $f_{x_i} = \frac{dP_{X_i}}{d\nu_i}$, $i \in [n]$.

	\begin{dukazin}
		„$\implies$“: použijeme charakterizaci nezávislosti složek pomocí distribuční funkce:
		$$ \int_{(-∞, ¦x]} \prod_{i=1}^n f_{X_i}(t_i) d(\nu_1 \otimes … \otimes \nu_n)(¦t) = \int_{-∞}^{x_1} … \int_{-∞}^{x_n} \prod_{i=1}^n f_{X_i}(t_i) d\nu_n(t_n) … d\nu_1(t_1) = \prod_{i=1}^∞ \int_{-∞}^{x_i} f_{x_i}(t_i) d\nu_i(t_i)\prod_{i=1}^n F_{X_i}(x_i) = F_{¦X}(¦x) = \int_{(-∞, ¦x} f_{¦X} (t_1, …, t_n) d(\nu_1 \otimes … \otimes \nu_n) (¦t) $$

		Takže $f_{¦x} = \prod_{i=1}^n f_{X_i}$ $\nu_1 \otimes … \otimes \nu_n$-skoro všude. „$\impliedby$“ dokážeme obdobně obráceným postupem.
	\end{dukazin}
\end{veta}

\begin{veta}
	Buď $\{X_i\}_{i \in I}$ systém nezávislých náhodných veličin a $g_i: ®R \rightarrow ®R, i \in I$ měřitelné funkce. Pak $\{g_i(X_i)\}_{i \in I}$.

	\begin{dukazin}
		Dokážeme z definice: Buď $J \subset I$ konečná neprázdná
		$$ P(\bigcap_{i \in J} \{g_i(X_i) \in B_i\}) = P(\bigcap_{i \in J} \{X_i \in g_i^{-1}(B_i)\}) = \prod_{i \in J} P(X_i \in g_i^{-1}(B_i)) = \prod_{i \in J} P(g_i(x_i) \in B_i), \qquad \forall B_i \in ©B. $$
	\end{dukazin}
\end{veta}

\section{Momenty náhodného vektoru}
\begin{definice}[Notace: střední hodnota náhodného vektoru]
	$$ ®E ¦X := \(®E X_1, …, ®E X_n\)^T. $$
\end{definice}

\begin{definice}[Kovariance, korelace]
	Buďte $X$, $Y$ náhodné veličiny na pravděpodobnostním prostoru $(\Omega, ©A, P)$. Pak kovariance $X$ a $Y$ je definovaná jako $\cov(X, Y) = ®E[(X - ®E X)(Y - ®E Y)]$. Korelace $X$ a $Y$ je definována jako $\cor(X, Y) = \frac{\cov(X, Y)}{\sqrt{\var X · \var Y}}$, pokud $\var X · \var Y > 0$.
\end{definice}

\begin{veta}[Hölderova nerovnost]
	Buďte $X_1$, $X_2$ náhodné veličiny na stejném pravděpodobnostním prostoru a nechť $®E |X_1|^p < ∞$, $®E |X_2|^q < ∞$, $p, q > 1$ a $\frac{1}{p} + \frac{1}{q} = 1$. Pak $®E |X_1·X_2| ≤ \(®E |X_1|^p\)^{\frac{1}{p}}·\(®E |X_2|^q\)^{\frac{1}{q}}$, a rovnost nastává, když $X_1 = c·X_2$ skoro jistě.

	\begin{dukazin}
		MA3.
	\end{dukazin}
\end{veta}

\begin{dusledek}
	$®E|X_1·X_2| ≤ \sqrt{®E X_1^2 · ®E X_2^2}$, takže $|\cov(X, Y)| ≤ \sqrt{\var X · \var Y}$ a $\cor (X, Y) \in [-1, 1]$. Navíc $|\cor(X, Y)| = 1 \Leftrightarrow X = a Y + b$ skoro jistě pro nějaké $a ≠ 0$, $b \in ®R$.
\end{dusledek}

\begin{veta}
	Buďte náhodné veličiny $X_1$ a $X_2$ nezávislé a $®E |X_i| < ∞$, $i = 1, 2$. Pak $®E |X_1·X_2| < ∞$ a platí $®E(X_1·X_2) = (®E X_1) · (®E X_2)$.

	\begin{dukazin}
		$$ ®E(X_1·X_2) = \int_{®R^2} x_1·x_2 dP_{¦X}(¦x) = \int_{®R^2} x_1·x_2 d\(P_{X_1} \otimes P_{X_2}\)(¦x) = \int_{®R} x_1 d P_{x_1} · \int_{®R} x_2 d P_{x_2} = (®E X_1)·(®E X_2), $$
		z Fubiniovy věty, pokud $®E |X_1·X_2| < ∞$. Uvažujme $\Phi_n(x_1, x_2) = |x_1|·1_{|x_1| ≤ n} · |x_2| 1_{|x_2 ≤ n|}$. Pak $®E \Phi_n(X_1, X_2) = $
		$$ \int_{®R^2} |x_1|·|x_2| 1_{|x_1| ≥ n} ·1_{|x_2| ≤ n} dP_{X_1} \otimes dP_{X_2} (x_1, x_2) = ®E\(|X_1| 1_{\{X_1 ≤ m\}}\)·®E\(|X_2| 1_{\{|X_2| ≤ n\}}\) ≤ ®E |X_1|·®E |X_2|. $$
		Takže $\Phi_n(X_1, X_2) \nearrow |X_1, X_2|$ z Léviho věty $®E |X_1·X_2|$ existuje a z poslední nerovnosti je $®E|X_1, X_2| ≤ ®E |X_1|·®E |X_2|$.
	\end{dukazin}
\end{veta}

\begin{veta}[P $\cov$ a $\var$ pro nezávislost náhodných veličin]
	Buďte $X_1, …, X_n$ nezávislé náhodné veličiny, $®E X_i^2 < ∞$, $\forall i \in [n]$. Pak $\cov(X_i, X_j) = 0$, $i ≠ j$. $\forall a_1, …, a_n \in ®R$ platí $\var\(\sum_{i=1}^n a_i X_i\) = \sum_{i=1}^n a_i^2 \var X_i$.

	\begin{dukazin}
		$$ \cov(X_i, X_j) = ®E[(X_i - ®E X_i)(X_j - ®E X_j)] = ®E X_i X_j - ®E X_i (®E X_j) - (®E X_i)®E X_j + ®E X_i + ®E X_k = ®E(X_i X_j) - (®E X_i)(®E X_j) = 0 $$
		z nezávislosti a předchozí věty.

		$$ \var\(\sum_{i=1}^n a_i X_i\) = ®E\(\sum_{i=1}^n a_i X_i - ®E\(\sum_{i=1}^n a_i X_i\)\)^2 = ®E\(\sum_{i=1}^n a_i(X_i - ®E X_i)\)^2 = \sum_{i=1}^n a_i^2 \var X_i + 2 \sum_{i=1}^n \sum_{j = i + 1}^n a_i a_j \cov(X_1, X_2) = \sum_{i=1}^n a_i^2 \var X_i + 0. $$
	\end{dukazin}
\end{veta}

% 16. 03. 2022

\begin{definice}[Nekorelované náhodné veličiny]
	Náhodné veličiny $X$ a $Y$ s $\cov(X, Y) = 0$ nazveme nekorelované.
\end{definice}

\begin{definice}[Varianční matice, korelační matice]
	Varianční matice $n$-rozměrného náhodného vektoru ¦X je matice $n \times n$ s prvky $a_{ij} = \cov(X_i, X_j)$, $i, j \in [n]$, tj.
	$$ \Var ¦X = ®E(¦X - ®E ¦X)(¦X - ®E ¦X)^T. $$
	Korelační matice $n$-rozměrného náhodného vektoru ¦X je matice $n \times n$ s prvky $a_{ij} = \cov(X_i, X_j)$, $i, j \in [n]$.
\end{definice}

\begin{veta}[O vlastnostech varianční matice]
	Buď $¦X = (X_1, …, X_n)^T$ náhodný vektor takový, že $\forall i \in [n]: ®E (X_i^2) < ∞$. Pak
	
	\begin{enumerate}
		\item $\Var ¦X$ je symetrická a pozitivně semidefinitní;
		\item pro libovolné $¦a \in ®R^m$ a matici $B$ typu $m \times n$ je $\Var(¦a + B¦X) = B (\Var ¦X) B^T$;
		\item $|\cov(X_i, X_j)| ≤ \sqrt{\var(X_i)·\var(X_J)}$, a rovnost nastává právě tehdy, když existují konstanty $a, b$, že $X_i = a + b X_j$ skoro jistě;
		\item jsou-li $X_1, …, X_n$ vzájemně nezávislé, pak $\Var ¦X$ je diagonální;
		\item $\Var ¦X$ je singulární $\Leftrightarrow$ existují $a_1, …, a_n \in ®R$, alespoň jedno nenulové, taková, že $\sum_{i=1}^n a_i X_i = k$ skoro jistě, kde $k$ je nějaká konstanta.
	\end{enumerate}

	\begin{dukazin}
		1. symetrická zřejmě, pro pozitivní semidefinitnost chceme dokázat, že $¦a (\Var ¦X) ¦a^T ≥ 0$ (rozepíšeme jako v minulé větě):
		$$ ¦a (\Var ¦X) ¦a^T = \sum_{i=1}^n \sum_{j=1}^n a_i a_j \cov(X_i, X_j) = \var(\sum_{i=1}^n a_i X_i) ≥ 0. $$
		
		Ve 2. snadně dokážeme $®E(B¦X) = B ®E ¦X$. Tedy
		$$ \Var(¦a + B¦X) = ®E(¦a + B¦X - ®E(¦a + B¦X))(¦a + B¦X - ®E(¦a + B®X))^T = ®E(B¦X - ®E(B¦X))(B¦X - ®E(B¦X))^T = ®E(B(¦X - ®E ¦X)(B(¦X - ®E ¦X))^T)  = B \Var(¦X) B^T. $$

		3. už jsme ukázali jako důsledek Hölderovy nerovnosti. Bod 4. je zřejmý z věty o kovarianci pro nezávislé náhodné veličiny.

		5. $\Var ¦X$ je singulární $\Leftrightarrow$ $\exists ¦a \in ®R^n$, $¦a ≠ ¦o$ tak, že $¦a (\Var ¦X) ¦a^T = 0$, ale $¦a (\Var ¦X) ¦a^T = \exists (¦a¦X - ®E ¦a ¦X)^2 = 0$ $\Leftrightarrow$ $¦a ¦X = ®E ¦a ¦X$ skoro jistě.
	\end{dukazin}
\end{veta}

\begin{veta}[O momentech výběrového průměru]
	Buďte $X_1, …, X_n$ nezávislé (nebo jen nekorelované) náhodné veličiny a buďte $®E X_i = \mu$, $\var X_i = \sigma^2$, $i \in [n]$, kde $\mu \in ®R$, $\sigma^2 ≥ 0$. Pak pro $\overline{X_n} = \frac{1}{n} \sum_{i=1}^n X_i$ platí $®E \overline{X_n} = \mu$ a $\var \overline{X_n} = \frac{\sigma^2}{n}$.

	\begin{dukazin}
		Rozepsáním z linearity střední hodnoty a z věty o vlastnostech varianční matice bod 2.
	\end{dukazin}
\end{veta}

% 21. 03. 2022

\section{Rozdělení transformovaného náhodného vektoru}
\begin{veta}
	Buďte $X, Y$ nezávislé náhodné veličiny a $\psi: ®R^2 \rightarrow ®R$ měřitelná. Pak náhodná veličina $U = \psi(X, Y)$ má distribuční funkci
	$$ G_U(u) = \int_{®R} \int_{\{y | \psi(x, y) ≤ u\}} dF_Y(y) dF_X(x) = \int_{®R} \int_{\{y | \psi(x, y) ≤ u\}} dP_Y(y) dP_X(x) = $$
	$$ G_U(u) = \int_{®R} \int_{\{x | \psi(x, y) ≤ u\}} dF_X(x) dF_Y(y) = \int_{®R} \int_{\{x | \psi(x, y) ≤ u\}} dP_X(x) dP_Y(y), \qquad \forall u \in ®R. $$

	\begin{dukazin}
		Víme, že $®E U = \int_{®R^2} \psi(x, y) d(P_X \otimes P_Y)(x, y)$. Použijeme Fubiniovu větu a vzorec použijeme pro náhodnou veličinu $\tilde{U}(u) = 1_{(U ≤ u)} = 1_{(\psi(X, Y) ≤ u)}$. To nám dá
		$$ G_U(u) = P(U ≤ u) = ®E 1_{(U ≤ u)} = \int_{®R} \int_{®R} 1_{(\psi(x, y) ≤ u)} dP_Y(y) dP_X(x). $$
	\end{dukazin}
\end{veta}

\begin{veta}[O rozdělení součtu]
	Buďte $X, Y$ nezávislé náhodné veličiny. Pak náhodná veličina $U = X + Y$ má distribuční funkci
	$$ F_U(u) = \int_{®R}F_x(u - y) dF_Y(y) = \int_{®R} F_Y(u - x) dF_X(x), \qquad \forall u \in ®R. $$

	\begin{dukazin}
		Dosazením do předchozí věty.
	\end{dukazin}
\end{veta}

\begin{definice}[Konvoluce]
	Buď $\psi(x, y) = x + y$. Pak $\psi(P_X \otimes P_Y)$ se nazývá konvoluce pravděpodobnostních rozdělení $P_X$ a $P_Y$. Buďte $F_X$ a $F_Y$ distribuční funkce. Pak $F_U$ definovaná jako $F_U(u) = \int_{®R} F_X(u - y) dF_y(y) = \int_{®R} F_y(u - x) dF_X(x)$ se nazývá konvoluce distribučních funkcí. Značíme $P_X * P_Y$, resp. $F_X * F_Y$.
\end{definice}

\begin{dusledek}[Věty o rozdělení součtu nezávislých náhodných veličin]
	Buďte $X, Y$ nezávislé náhodné veličiny a buďte obě absolutně spojité. Pak $U = X + Y$ je také absolutně spojitá s hustotou $f_U(u) = \int_{®R}f_X(u - y) dy = \int_{®R} f_Y(u - x)f_X(x) dx$, $u \in ®R$.

	\begin{dukazin}
		Dosazením.
	\end{dukazin}
\end{dusledek}

\begin{poznamka}
	Pro ne-nezávislá $X, Y$ lze snadno odvodit analogický vzorec (s jinou než součinovou mírou).
\end{poznamka}

\begin{veta}
	Buďte nezávislé náhodné veličiny $X$ a $Y$ čítací (tj. $P(X \in ®N_0) = 1 = P(Y \in ®N_0)$). Pak $U = X + Y$ je také čítací náhodná veličina a $P(U = u) = \sum_{n=0}^u P(X = n)·P(Y = u - n)$, $u \in ®N_0$.

	\begin{dukazin}
		Snadno z věty o úplné pravděpodobnosti.
	\end{dukazin}
\end{veta}

\begin{veta}[O transformaci hustot]
	Buď ¦X $n$-rozměrný absolutně spojitý náhodný vektor s hustotou $f_{¦X}$. Buď $S_{¦X}$ otevřená množina taková, že $P(¦X \in S_{¦X}) = 1$, a $g: S_{¦X} \rightarrow ®R^n$ difeomorfismus. Pak rozdělení náhodného vektoru $¦Y = g(¦X)$ má vzhledem k $\lambda^n$ hustotu $f_{¦Y}(¦y) = f_{¦X}(g^{-1}(¦y) · |\Jac g^{-1}(¦y)| !_{g(S_{¦X})}(¦y)$, $¦y \in ®R^n$.

	\begin{dukazin}
		Z věty o obrazu míry víme, že $P_{¦X}(A) = P_{¦Y}(g(A))$, $\forall A \in ©B^n$, resp. $\forall g(A) \in ©B^n$. Pokud existuje hustota $f_{¦Y}$, pak je to také rovno $\int_{g(A)} f_{¦Y}(¦y) d¦y$. Z předpokladů máme, že $g^{-1}$ je difeomorfismus na $g(S_{¦X})$. Použijeme větu o substituci s volbami $h = f_{¦X}$, $\phi = g^{-1}$, $M = g(S_{¦x})$ a $N = A$ a dostaneme
		$$ P_{¦X}(A) = \int_A f_{¦X}(¦x) d¦x = \int_{g(A)} f_{¦X}(g^{-1}(¦y)) |\Jac g^{-1}(¦y)| d¦y, $$
		pro každou $A \subset g^{-1}(g(S_{¦X})) = S_{¦X}$, resp. $\forall g(A) \subset g(S_{¦X})$. Levý integrál zřejmě existuje, tedy existuje i pravý. Z předpokladů víme $\int_{S_{¦X}^c} f_{¦X}(¦x) d¦x = P(X \in S_{¦X}^c) = 0$ a také $\int_{(g(S_{¦x}))^c} f_{¦Y}(¦y) d¦y = 0$ a oba integrály jsou nulové i pro všechny podmnožiny $S_{¦X}^c$, resp. $(g(S_{¦x}))^c$. Takže $\forall B \in ©B^n$ a funkci $f_{¦y}$ ze znění dostáváme:
		$$ P_{¦Y}(B) = P_{¦Y}(B \cap g(S_{¦X})) + P_{¦Y}(B \setminus g(S_{¦X})) = P_{¦X}(g^{-1}(B \cap g(S_{¦X}))) + 0 = \int_{B \cap g(S_{¦X})} f_{¦Y} d¦y + 0 = \int_{B \cap g(S_{¦X})} f_{¦Y} d¦y = \int_B f_{¦y} d¦y. $$
		A tedy $f_{¦Y}$ je opravdu hustota ¦Y.
	\end{dukazin}
\end{veta}

% 23. 03. 2022

\section{Mnoharozměrné normální rozdělení}
\begin{definice}[Mnoharozměrné normální rozdělení]
	Buď $¦Z = (Z_1, …, Z_r)^T$, $r \in ®N$, $r$-rozměrný váhový vektor, kde $Z_i$ jsou vzájemně nezávislé a $Z_i \sim N(0, 1)$, $i \in [r]$. Buď $A_{n \times r}$, $n \in ®N$ matice a $\mu = (\mu_1, …, \mu_n) \in ®R^n$ pevný vektor. Náhodný vektor definovaný jako $¦X = A¦Z + \mu$ má $n$-rozměrné normální rozdělení s parametry $\mu$ a $\Sigma = A A^T$. Značíme $N_n(\mu, \Sigma)$.
\end{definice}

\begin{dusledek}
	\ 
	\begin{itemize}
		\item $¦Z \sim N_r(), I_n)$.
		\item $®E ¦X = \mu$.
		\item Pro $k \in ®N$ a matici $B_{k \times n}$ platí, že $¦Y = B ¦X \sim N_k(B_\mu, B \Sigma B^T)$.
		\item Speciálně pro vektor $¦c = (c_1, …, c_n) \in ®R^n$ má $¦c¦X$ jednorozměrné normální rozdělení $N(¦c \mu, ¦c \Sigma ¦c^T)$.
	\end{itemize}

	\begin{dukazin}
		Byl na přednášce, ale jednoduchý.
	\end{dukazin}
\end{dusledek}

\begin{veta}[O hustotě $n$-rozměrného normálního rozdělení]
	Buď ¦X náhodný vektor s rozdělením $N_n(\mu, \Sigma)$, kde $\Sigma$ je regulární matice. Pak $P_{¦X} \ll \lambda^n$ a
	$$ f_{¦X}(¦x) = \frac{1}{(2\pi)^{\frac{n}{2}}\sqrt{\det \Sigma}} e^{-\frac{1}{2} (¦x - \mu)^T \Sigma^{-1} (¦x - \mu)}, \qquad ¦x \in ®R^n. $$

	\begin{dukazin}
		Nejdříve buď $¦X \sim N_n(0, I_n)$. Pak $X_i = Z_i$ z definice a víme tedy, že $X_i \sim N(0, 1)$ a $X_i$ jsou vzájemně nezávislé, tedy
		$$ f_{¦X}(¦x) = \prod_{i=1}^n \frac{1}{\sqrt{2 \pi}} e^{-\frac{x_i^2}{2}}, \qquad ¦x \in ®R^n. $$

		Následně buď $\Sigma$ pozitivně definitní, $\mu \in ®R^n$. Pak $\exists A_{n \times n}$ taková, že $\sigma = A A^T$ a $A$ je regulární. Položme $¦Y = A¦X + \mu$, kde $¦X$ je vektor ze začátku důkazu. Tedy $¦Y \sim N_n(\mu, A^T A = \Sigma)$. Použijeme větu o transformaci hustot na odvození $f_{¦Y}$:

		Mějme zobrazení $g: ®R^n \rightarrow ®R^n$, $g(¦x) = A¦x + \mu$. $g$ je difeomorfismus na $®R^n$, $|\Jac g| = |\det A| ≠ 0$, tedy můžeme volit $S_{¦X} = ®R^n$, $g(S_{¦X}) = ®R^n$, $g^{-1}(¦y) = A^{-1}(¦y - \mu)$, tedy:
		$$ f_{¦Y}(¦y) = \frac{1}{(2\pi)^{\frac{n}{2}}} · \frac{1}{|\det A|} · e^{-\frac{1}{2}(A^{-1}(¦y - \mu))^T(A^{-1}(¦y - \mu))} = $$
		$$ = \frac{1}{(2\pi)^{\frac{n}{2}}} · \frac{1}{\sqrt{\det \Sigma}}·e^{-\frac{1}{2} (¦y - \mu)^T \sigma^{-1} (¦y - \mu)}, \qquad ¦y \in ®R^n. $$
	\end{dukazin}
\end{veta}

\begin{dusledek}[O marginálních rozděleních v $N_n$]
	Buď ¦X náhodný vektor $\sim N_n(\mu, \Sigma)$. Pak marginální rozdělení $X_i$ je $N(\mu_i, \sigma_i^2)$, kde $\sigma_i^2 = \Sigma_{i, i} = \var X_i$. A podvektor $(X_i, X_j)^T$, $i ≠ j$, má rozdělení $N_2\(\begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} \sigma_i^2 & \rho_{ij}\sigma_i\sigma_j \\ \rho_{ij} \sigma_i\sigma_j & \sigma_j^2 \end{pmatrix}\)$, kde $\rho_{ij} = \cor(X_i, X_j)$.

	\begin{dukazin}
		Použijeme předchozí důsledek definice, čtvrtý bod pro první část a třetí bod pro druhou část.
	\end{dukazin}
\end{dusledek}

\begin{tvrzeni}
	Nechť máme náhodný vektor $(X, Y)^T \sim N_2$. Pak mají $X, Y$ jednorozměrné normální rozdělení. Pokud navíc $\cov(X, Y) = 0$, pak jsou $X$ a $Y$ nezávislé.
\end{tvrzeni}

% 28. 03. 2022

\begin{definice}[$\chi^2$-rozdělení]
	Buďte $X_1, …, X_n$, $n \in ®N$ nezávislé náhodné veličiny s rozdělením $N(0, 1)$. Pak náhodná veličina $Y_n = \sum_{i=1}^n X_i^2$ má $\chi^2$-rozdělení o $n$ stupních volnosti (značíme $\chi_n^2$), s hustotou
	$$ g_n(y) = \frac{y^{\frac{n}{2} - 1}}{2^{\frac{n}{2}} \Gamma(\frac{n}{2})}e^{-\frac{y}{2}} 1_{y > 0}, \qquad y \in ®R. $$

	\begin{poznamka}
		Střední hodnota je $n$, rozptyl $2n$.
	\end{poznamka}
\end{definice}

\begin{definice}[Studentovo rozdělení]
	Buďte $X \sim N(0, 1)$ a $Y \sim \chi_n^2$ nezávislé náhodné veličiny. Pak náhodná veličina $T = \frac{X}{\sqrt{\frac{Y}{n}}}$ má studentovo $t_n$ rozdělení (neboli $t$-rozdělení o $n$ stupních volnosti) s hustotou
	$$ h_n(t) = \frac{\Gamma(\frac{n+1}{2})}{\sqrt{\phi n} \Gamma(\frac{n}{2})} \(1 + \frac{t^2}{n}\)^{-\frac{n+1}{2}}, \qquad t \in ®R. $$

	\begin{poznamka}
		Střední hodnota je 0, pokud $n > 1$. Obecně $t_n$-rozdělení má konečné momenty až do řádu $(n-1)$ včetně.

		Pro $n = 1$ se toto rozdělení nazývá Cauchyho rozdělení (protože se chová jinak než ostatní studentova).
	\end{poznamka}
\end{definice}

\section{Limitní věty}
\begin{definice}[Značení]
	Buďte $A_1, A_2, …$ náhodné jevy ze $\sigma$-algebry ©A. Značíme
	$$ \limsup_{n \rightarrow ∞} A_n = \bigcap_{n=1}^∞ \bigcup_{k=n}^∞ A_k, \qquad \liminf_{n \rightarrow ∞} A_n = \bigcup_{n=1}^∞ \bigcap_{k=n}^∞ A_k. $$
\end{definice}

\begin{veta}[Cantelli]
	Buďte $A_1, A_2, …$ náhodné jevy z ©A. Pokud $\sum_{n=1}^∞ P(A_n) < ∞$, pak $P(\limsup_{n \rightarrow ∞} A_n) = 0$.

	\begin{dukazin}
		$$ P(\limsup_{n \rightarrow ∞} A_n) = P(\bigcap_{n=1}^∞ \bigcup_{k=n}^∞ A_k)  = \lim_{n \rightarrow ∞} P(\bigcup_{k=n}^∞ A_k) = \lim_{n \rightarrow ∞} P(\bigcup_{k=n}^∞ A_k) ≤  \lim_{n \rightarrow ∞} \sum_{k=n}^∞ P(A_k) = 0. $$
	\end{dukazin}
\end{veta}

\begin{dusledek}
	$$ P((\limsup_{n\rightarrow ∞} A_n)^c) = 1 = P(\liminf_{n \rightarrow ∞} A_n^c). $$
\end{dusledek}

\begin{veta}[Borelova, Borelův 0-1 zákon]
	Buďte $A_1, …$ nezávislé náhodné jevy z ©A. Pak
	$$ \sum_{n=1}^∞ P(A_n) < ∞ \Leftrightarrow P(\limsup_{n \rightarrow ∞} A_n) = 0, $$
	$$ \sum_{n=1}^∞ P(A_n) = ∞ \Leftrightarrow P(\limsup_{n \rightarrow ∞} A_n) = 1. $$

	\begin{dukazin}
		Neb nic jiného než $\sum = ∞$ nebo $\sum < ∞$ nastat nemůže, stačí ukázat dvakrát $\implies$. „První $\implies$“ je Cantelliho věta.

		„Druhá $\implies$“: Platí $P(\limsup_{n \rightarrow ∞} A_n) = 1 - P((\limsup_{n \rightarrow ∞} A_n)^c) = 1 - P(\liminf_{n \rightarrow ∞} A_n^c)$. Počítejme
		$$ P(\liminf_{n \rightarrow ∞} A_n^c) = P(\bigcup_{n=1}^∞ \bigcap_{k=n}^∞ A_k^c) = \lim_{n \rightarrow ∞} P(\bigcap_{k=n}^∞ A_n^c) = \lim_{n \rightarrow ∞} \lim_{N \rightarrow ∞} P(\bigcap_{k=n}^N A_n^c) = \lim_{n \rightarrow ∞} \lim_{N \rightarrow ∞} \prod_{k=n}^N P(A_k^c) ≤ \lim_{n \rightarrow ∞} \lim_{N \rightarrow ∞} \prod_{k=n}^N e^{-P(A_k)} = \lim_{n \rightarrow ∞} \lim_{N \rightarrow ∞} e^{\sum_{k=n}^N P(-A_n)} = \lim_{n \rightarrow ∞} 0 = 0. $$
	\end{dukazin}
\end{veta}

% 30. 03. 2022

\begin{definice}[Konvergence v pravděpodobnosti]
	Buďte $Y_1, Y_2, …$ náhodné veličiny na $(\Omega, ©A, P)$. Posloupnost $\{Y_n\}_{n = 1}^∞$ konverguje v pravděpodobnosti k náhodné veličině $Y$, značíme $Y_n \overset{P}{\rightarrow} Y$, pokus $P(|Y_n - Y| > \epsilon) \rightarrow 0$, $\forall \epsilon > 0$.

	\begin{poznamkain}
		Je to konvergence podle míry. Je ekvivalentní
		$$ \forall \epsilon > 0: P(|\frac{1}{n} \sum_{i=1}^n Y_i - ®E Y| ≤ \epsilon) \rightarrow 1? $$
	\end{poznamkain}
\end{definice}

\begin{definice}[Konvergence skoro jistě]
	Buďte $Y_1, …$ náhodné veličiny na $(\Omega, ©A, P)$. Posloupnost $\{Y_n\}_{n=1}^∞$ konverguje skoro jistě k náhodné veličině $Y$, značíme $Y_n \overset{\text{s.j.}}\rightarrow Y$, pokud $P(\{\omega \in \Omega | \lim_{n \rightarrow ∞}Y_n(\omega) = Y(\omega)\}) = 1$.

	\begin{poznamkain}
		Je to konvergence skoro všude.
	\end{poznamkain}
\end{definice}

\begin{veta}[O konvergenci v pravděpodobnosti a součtu]
	Buďte $Y_1, …$ náhodné veličiny na $(\Omega, ©A, P)$ náhodné veličiny a $Z_1, …$ náhodné veličiny také na $(\Omega, ©A, P)$. Pak
	$$ (Y_n \overset{P}\rightarrow 0) \land (Z_n \overset{P}\rightarrow 0) \implies (Y_n + Z_n \overset{P}\rightarrow 0). $$

	\begin{dukazin}
		$$ P(|Y_n + Z_n| > \epsilon) ≤ P(|Y_n| > \frac{\epsilon}{2}) + P(|Z_n| > \frac{\epsilon}{2}) \rightarrow 0, \qquad \forall \epsilon > 0. $$
	\end{dukazin}
\end{veta}

\begin{veta}[O konvergenci v pravděpodobnosti a násobení]
	Za stejných předpokladů jako výše s přidáním $\{a_n\}_{n=1}^∞$ je reálná omezená posloupnost, je $Y_n \overset{P}\rightarrow 0 \implies a_nY_n \overset{P}\rightarrow 0$.

	\begin{dukazin}
		$$ P(|a_n Y_n| > \epsilon) ≤ P(|Y_n| > \frac{\epsilon}{c}) \rightarrow 0. $$
	\end{dukazin}
\end{veta}

\begin{veta}[O spojité transformaci a konvergence]
	Buďte $Y_1, Y_2, …$ n. v. na $(\Omega, ©A, P)$ a $g: ®R \rightarrow ®R$ funkce spojitá na otevřené množině $S_Y$ takové, že $P(Y \in S_Y) = 1$. Pak
	$$ Y_n \overset{P} \rightarrow Y \implies g(Y_n) \overset{P}\rightarrow g(Y), $$
	$$ Y_n \overset{\text{s. j.}}\rightarrow Y \implies g(Y_n) \overset{\text{s. j.}}\rightarrow g(Y). $$

	\begin{dukazin}
		První implikace viz literatura. Druhá: Buď $N \subset \Omega$ taková, že $Y_n(\omega) \rightarrow Y(\omega)$, $\forall \omega \in \Omega \setminus N$. Označíme $M = (\Omega \setminus N) \cap Y^{-1}(S_Y)$. Pak $P(M) = 1$. A $\forall \omega \in M$ platí, že $g$ je spojitá na otevřeném okolí $Y(\omega)$, a tedy z $Y_n(\omega) \rightarrow Y(\omega)$ dostaneme $g(Y_n(\omega)) \rightarrow g(Y(\omega))$ z věty o limitě spojitě transformované posloupnosti v ®R.
	\end{dukazin}
\end{veta}

\begin{veta}[Čebyševův slabý zákon velkých čísel]
	Buďte $X_1, X_2, …$ vzájemně nezávislé náhodné veličiny definované na tomtéž $(\Omega, ©A, P)$ splňujíxí $®E X_n^2 < ∞$, $n \in ®N$, a $\lim_{n \rightarrow ∞} \(\frac{1}{n^2} \sum_{i=1}^n \var X_i\) = 0$. Potom platí
	$$ |\overline{X\_n} - \overline{®E X_n}| \overset{P}{\rightarrow} 0. $$

	\begin{dukazin}
		Buď $\epsilon > 0$ pevné.
		$$ P(|\overline{X_n} - \overline{®E X_n}| > \epsilon) ≤ \frac{\var(\overline{X_n} - \overline{®E X_n})}{\epsilon^2} = \frac{\var \overline{X_n}}{\epsilon^2} = \frac{\var \frac{1}{n} \sum_{i=1}^n X_i}{\epsilon^2} = \frac{1}{n^2} \frac{\sum_{i=1}^n \var X_i}{\epsilon^2} = \frac{\frac{1}{n^2} \sum_{i=1}^n \var X_i}{\epsilon^2} \rightarrow 0. $$
	\end{dukazin}
\end{veta}

% 04. 04. 2022

TODO!!! (Chyběl jsem)

% 06. 04. 2022

TODO!!! (Chyběl jsem)
\begin{veta}[Silný zákon velkých čísel pro iid]
	Buď $\{X_n\}_{n=1}^∞$ posloupnost iid náhodných veličin. Pak
	$$ (\exists \mu \overline{X_n} \overset{\text{skoro jistě}}\rightarrow \mu) \Leftrightarrow ®E |X_1| < ∞. $$

	\begin{dukazin}
%
% 11. 04. 2022
%
		„$\implies$“: $X_n = n · \overline{X_n} - (n-1)·\overline{X_{n-1}}$, $\forall n ≥ 2$,
		$$ \frac{X_n}{n} = \overline{X_n} - \frac{n-1}{n} \overline{X_{n-1}}. $$

		Tedy $\frac{X_n}{n} \rightarrow 0$ skoro jistě. Takže $P(\frac{X_n}{n} ≥ 1, \text{ pro nekonečně mnoho $n$}) = P(\limsup \frac{|X_n|}{n} ≥ 1) = 0$.

		Z Borelovy věty máme, že jevy $|X_n| ≥ n$ jsou vzájemně nezávislé, neb $\{X_n\}_{n=1}^∞$ jsou vzájemně nezávislé.
		$$ \sum_{n=1}^∞ P(|X_n| ≥ n) < ∞, \qquad \sum_{n=1}^∞ P(|X_1| ≥ n) < ∞. $$
		Nyní z předchozího lemmatu máme $®E |X_1| < ∞$.

		Navíc z jednoznačnosti limity skoro jistě máme, že $\mu = ®E X_1$.
	\end{dukazin}
\end{veta}

\begin{definice}[Konvergence v distribuci]
	Buď $Y_n$ posloupnost náhodných veličin na libovolných pravděpodobnostních prostorech. Řekneme, že náhodné veličiny $Y_n$ konvergujív distribuci (značíme $\overset{D}\rightarrow$), pokud $\lim_{n \rightarrow ∞} F_{Y_n} (x) = F_Y(x)$ $\forall x \in ®R$, které jsou body spojitosti $F_Y$.

	\begin{poznamkain}
		Konvergence v distribuci mluví o konvergenci měr $P_{Y_n}$, nikoliv $Y_n$.
	\end{poznamkain}
\end{definice}

\begin{veta}[Centrální limitní věta (Ljapunov 1901, Lévy, Lindenberg 1922)]
	Buďte $X_1, X_2, X_3, …$ nezávislé, stejně rozdělené náhodné veličiny s $®E X_1 = \mu \in ®R$ a $0 < \var X_1 = \sigma^2 < ∞$. Potom pro náhodné veličiny $Z_n = \frac{\sum_{n=1}^n X_i - \mu}{\sqrt{n \sigma^2}}$ platí $Z_n \overset{D}\rightarrow Z$, kde $Z \sim N(0, 1)$.
\end{veta}

% 13. 04. 2022

\begin{lemma}[Ekvivalentní charakterizace $\overset{D}\rightarrow$]
	Posloupnost náhodných veličin $\{Y_n\}_{n=1}^∞$ splňuje $Y_n \overset{D}\rightarrow Y$ právě tehdy, když $®E h(Y_n) \rightarrow ®E h(Y)$ pro každou spojitou omezenou funkci $h: ®R \rightarrow ®R$.

	\begin{dukazin}
		Důkaz je technický a byl vynechán.
	\end{dukazin}
\end{lemma}

\begin{dukaz}[Centrální limitní věty]
	BÚNO $®E X_i = 0$ a $\var X_i = 1$, $\forall i \in ®N$, (jinak bychom následující použili pro $\tilde{X}_i = \frac{X_i - \mu}{\sqrt{\sigma^2}}$). Z předchozího lemmatu nám stačí ukázat, že $®E h(Z_n) \rightarrow ®E h(Z)$ $\forall h$ 2krát spojitě diferencovatelné, stejnoměrně spojité a s omezenými prvními a druhými derivacemi.

	Buďte $\{Y_i\}_{i=1}^∞$ nezávislé náhodné veličiny $\sim N(0, 1)$ nezávislé s $\{X_i\}_{i=1}^∞$. Pak $T_n = \frac{\sum_{i=1}^∞ Y_n - 0}{\sqrt{n·1}} \sim N(0, 1)$. Teď chceme ukázat $|®E h(Z_n) - ®E h(T_n)| \rightarrow 0$ pro každé $h$ jako výše.

	Označíme si $X_{i, n} = \frac{X_i}{\sqrt{n}}$, $Y_{i, n} = \frac{Y_i}{\sqrt{n}}$, $W_{i, n} = \sum_{j=0}^i Y_{j, n} + \sum_{j=i+1}^n X_{i, n}$. Platí
	$$ h(Z_n) - h(T_n) = \sum_{i=1}^n (h(W_{i, n} + X_{i, n}) - h(W_{i, n} + Y_{i, n})), \qquad \text{neboť } W_{i, n} + X_{i, n} = W_{i + 1, n} + Y_{i-1, n}, \qquad 1 ≤ i ≤ n. $$
	Nyní použijeme Taylorův rozvoj na omezení jednotlivých rozdílů. Platí
	$$ h(W_{i, n} + X_{i, n}) = h(W_{i, n}) + h'(W_{i, n})X_{i, n} + \frac{1}{2} h'' (W_{i, n}) X_{i, n}^2 + R_{x, i, n}, $$
	$$ R_{x, i, n} = \frac{1}{2 X_{i, n}^2 (h''(W_{i, n} + \zeta X_{i, n}) - h''(W_{i, n}))}, \qquad 0 ≤ \zeta ≤ 1. $$
	A také platí $|R_{x, i, n}| ≤ X_{i, n}^2 ||h''||_∞$ a také platí $\forall \epsilon > 0\ \exists \delta > 0: |R_{x, i, n}| ≤ X_{i, n}^2 \epsilon$, pro $|X_{i, n}| < \delta$ ze stejnoměrné spojitosti $h''$. Dohromady $|R_{x, i, n}| ≤ X_{i, n}^2(\epsilon · 1_{(|X_{i, n}| < \delta)} + ||h''||_∞ 1_{(|X_{i, n}| ≥ \delta)})$. Obdobně vše pro $Y_{i, n}$.

	Dosadíme Taylorův rozvoj a odhady do teleskopické sumy výše a spočítáme střední hodnotu:
	$$ ®E(h(Z_n) - h(T_n)) = \sum_{i=1}^n \[®E (h(W_{i, n}) - h(W_{i, n})) + ®E h'(W_{i, n})(X_{i, n} - Y_{i, n}) + ®E \frac{1}{2} h''(W_{i, n})(X_{i, n}^2 - Y_{i, n}^2) + ®E R_{x, i, n} - ®E R_{y, i, n}\] = 0 + 0 + 0 + …, $$
	neboť první člen je zřejmě nula, druhý člen je střední hodnota součinu nezávislých veličin, tedy součin středních hodnot a $X, Y$ mají stejnou střední hodnotu a střední hodnota $W_{i, n}$ existuje, tedy i druhý člen je 0. Třetí člen je úplně totéž, jen $X, Y$ mají shodný rozptyl.
	$$ |®E (h(Z_n) - h(T_n))| ≤ \sum_{i=1}^n ®E(|R_{x, i, n}| + |R_{y, i, n}|) ≤ \sum_{i=1}^n (\epsilon ®E(X_{i, n}^2 + Y_{i, n}^2) + ||h''||_∞ ®E(X_{i, n}^2 1_{(|X_{i, n}| ≥ \delta)} + Y_{i, n}^2 1_{(|Y_{i, n}| ≥ \delta)})) = $$
	$$ = 2\epsilon + \sum_{i=1}^n ||h''||_∞ (®E \frac{X_i^2}{n} 1_{|X_i|} > \delta·\sqrt{n} + ®E \frac{Y_i^2}{n} 1_{(|Y_i| ≥ \delta·\sqrt{n})}) = $$
	$$ = 2\epsilon + ||h''||_∞(®E X_1^2 1_{(|X_1| ≥ \delta \sqrt{n}) + ®E Y_1^2 1_{(|Y_1| ≥ \delta \sqrt{n})}}), $$
	A neboť $®E X_1^2 1_{(|X_1| ≥ \delta \sqrt{n})} = ®E X_1^2 - ®E X_1^2 1_{(|X_1| < \delta \sqrt{n})} \rightarrow ®E X_1^2 - ®E X_1^2 = 0$ z Lebesgueovy věty.Tak
	$$ |®E (h(z_n) - h(t_n))| \rightarrow 2\epsilon. $$
	Tedy $\forall \epsilon > 0$ $\exists n: |®E h(Z_n) - ®E h(T_n)| < 3\epsilon$, tedy teleskopická suma jde k nule, tudíž $|®E h(Z_n) - ®E h(T_n)| \rightarrow 0$ $\forall h$ splňující podmínky výše.
\end{dukaz}

% 20. 04. 2022

\begin{veta}[De Moivre-Laplaceova centrální limitní věta]
	Buďte $Y_n \sim \Binom(n, p)$ náhodné veličiny pro $n \in ®N$ a $p \in (0, 1)$. Pak
	$$ \frac{Y_n - n·p}{\sqrt{n·p·(1 - p)}} \overset{D}\rightarrow N(0, 1). $$

	\begin{dukazin}[Dá se napočítat a umlátit Stirlingem, ale pro trénink]
		Buď $\{X_i\}_{i=1}^∞$ posloupnost iid s $X_i \sim \Alt(p)$. Pak $\sum_{i=1}^n X_i = Y_n \sim \Binom(n, p)$. A také $\{X_i\}$ splňuje předpoklady CLV. A tedy
		$$ Z_n = \frac{\sum_{i=1}^n (X_i - \mu)}{\sqrt{n·\sigma^2}} = \frac{\sum_{i=1}^n (X_i - p)}{\sqrt{n·p·(1 - p)}} \overset{D}\rightarrow N(0, 1). $$
		Tudíž také
		$$ \frac{-n·p + \sum_{i=1}^n X_i}{\sqrt{n·p·(1 - p)}} \overset{D} \rightarrow N(0, 1). $$
		Takže pro $Y_n$ věta platí. Pro jiné $V_n \sim \Binom(n, p)$ platí toto také, neboť konvergence v distribuci závisí pouze na rozdělení.
	\end{dukazin}
\end{veta}

\begin{lemma}[O spojité transformaci a konvergenci v distribuci]
	Buď $\{X_i\}_{i=1}^∞$ posloupnost nezávislých veličin taková, že $X_n \overset{D} \rightarrow X$, $X$ náhodná veličina, a $g: ®R \rightarrow ®R$ buď spojitá funkce. Nechť $Y_n = g(X_n)$, $n \in ®N$, pak $Y_n \overset{D} \rightarrow Y$, kde $Y = g(X)$.

	\begin{dukazin}
		Použijeme charakterizaci konvergenci v distribuci: ukážeme, že $\lim_{n \rightarrow ∞} ®E h(Y_n) = ®E h(y)$ pro každou $h$ omezenou spojitou funkci z ®R do ®R. Ale $®E h(Y_n) = ®E h(g(X_n))$ a $h \circ g$ je spojitá a omezená funkce z ®R do ®R.

		Tedy protože $X_n \overset{D} \rightarrow X$, musí $®E hg(X_n) \rightarrow ®E h(g(X))$.
	\end{dukazin}

	\begin{veta}
		Buď $\{X_n\}_{n=1}^∞$ posloupnost náhodných veličin definovaných na $(\Omega, ©A, P)$ a $X$ náhodná veličina na $(\Omega, ©A, P)$. Pak $(X_n - X) \overset{P} \rightarrow 0$ $\implies$ $X_n \overset{D} \rightarrow X$.

		\begin{dukazin}
			Vynechán (viz TP1).
		\end{dukazin}

		\begin{upozorneni}
			Obráceně to neplatí.
		\end{upozorneni}
	\end{veta}

	\begin{veta}[Cramér-Slucký]
		Buďte $\{X_n\}_{n=1}^∞$, $\{Y_n\}_{n=1}^∞$, $X$ náhodné veličiny a nechť $X_n \overset{D} \rightarrow X$ a $Y_n \overset{P} \rightarrow a \in ®R$. Pak
		$$ X_n + Y_n \overset{D} \rightarrow (X + a) \land X_n·Y_n \overset{D} \rightarrow X·a. $$

		\begin{dukazin}
			Vynechán.
		\end{dukazin}
	\end{veta}
\end{lemma}

% 25. 04. 2022

\part{Statistika}
\begin{definice}[Empirická distribuční funkce]
	Empirická distribuční funkce je definována jako $F_n(x, \omega) = \frac{1}{n} \sum_{k=1}^n 1_{(X_k ≤ x)}(\omega) = \frac{1}{n} \sum_{k=1}^n 1_{(X_k(\omega) ≤ x)}$.
\end{definice}

\begin{definice}[Náhodný výběr, rozsah výběru]
	Buďte $X_1, …, X_n$ nezávislé, stejně rozdělené náhodné veličiny se stejným rozdělením $P_X$ s distribuční funkcí $F$. Pak $(X_1, …, X_n)$ nazveme náhodný výběr z rozdělení s distribuční funkcí $F$ (resp. z rozdělení $P_X$). $n$ je tzv. rozsah výběru.
\end{definice}

\begin{definice}[Model]
	Modelem pro pozorování $X_1, …, X_n$ rozumíme předem stanovenou množinu rozdělení ©F, do níž neznámé rozdělení $P_X$, resp. jeho distribuční funkce $F$ patří.
\end{definice}

\section{Bodový odhad}
\begin{definice}[Parametrická funkce]
	(Máme náhodný výběr $(X_1, …, X_n)$ z modelu $©F = \{F_\theta | \theta \in \Theta\} = \{P_\theta | \theta \in \Theta\}$, kde $\Theta$ je Borelovská $\subset ®R^d$. Chceme odhadnout $\theta$ nebo $g(\theta)$.)

	Borelovsky měřitelné zobrazení $g: \theta \rightarrow ®R$ nazveme parametrickou funkcí.
\end{definice}

\begin{definice}[Bodový odhad paramterické funkce]
	Bodový odhad $\phi_n$ parametrické funkce $g(\theta)$ je borelovsky měřitelné zobrazení $\phi_n: ®R^n \rightarrow ®R$, jehož předpis nezávisí na $\theta$ ( a tedy ani na $P_\theta$ či $F_\theta$) a jehož definiční obor obsahuje obor hodnot $(X_1, …, X_n)$
\end{definice}

\begin{definice}[Nestranný bodový odhad]
	Bodový odhad $\phi_n$ parametrické funkce $g(\theta)$ se nazývá nestranný, pokud $\forall \theta \in \Theta$ platí $®E_\theta \phi_n(X_1, …, X_n) = g(\theta)$. ($®E_\theta$ značí, že ®E počítáme vzhledem k rozdělení $P_\theta^n$ neboli $X_i \sim P_\theta$.)
\end{definice}

\begin{definice}[Silně konzistentní posloupnost bodových odhadů]
	Posloupnost bodových odhadů $\phi_n$ parametrické funkce $g(\Theta)$ se nazývá silně konzistentní, pokud $\forall \theta \in \Theta$ platí $P(\lim_{n \rightarrow ∞} \phi(X_1, …, X_n) = g(\theta)) = 1$.
\end{definice}

% 27. 04. 2022

\begin{definice}[Slabě konzistentní posloupnost bodových odhadů]
	Posloupnost bodových odhadů $\phi_n$ parametrické funkce $g(\Theta)$ se nazývá slabě konzistentní, pokud $\forall \theta \in \Theta$ platí $\phi(X_1, …, X_n) \overset{P}\rightarrow g(\theta)$.
\end{definice}

\begin{definice}[Statistika, výběrový průměr, výběrový rozptyl]
	Statistikou nazveme libovolnou borelovsky měřitelnou funkci $T_n: ®R^n \rightarrow ®R$, jejíž definiční obor je obor hodnot náhodného výběru ($X_1, …, X_n$).

	Statistika $\overline{X_n} := \frac{1}{n} \sum_{i=1}^n X_i$ se nazývá výběrový průměr. Statistika $S_n^2 := \frac{1}{n - 1} \sum_{k=1}^n (X_k - \overline{X_n})^2$ si nazývá výběrový rozptyl.
\end{definice}

\begin{veta}[O výběrovém průměru a výběrovém rozptylu]
	Buď $X_1, …, X_n$ náhodný výběr z rozdělení se střední hodnotou $\mu = ®E X \in ®R$ a rozptylem $\sigma^2 = \var X < +∞$. Pak výběrový průměr $\overline{X_n}$ je nestranný a konzistentní odhad $\mu$ a výběrový rozptyl $S_n^2$ je nestranný a konzistentní odhad $\sigma^2$.

	\begin{dukazin}
		Nestrannost $\mu$:
		$$ ®E \overline{X_n} = ®E (\frac{1}{n} \sum_{i=1}^n X_i) = \frac{1}{n} · n · ®EX_1 = \mu. $$
		Konzistence $\mu$ plyne ze silného zákona velkých čísel.

		Pro $S_n^2$ upravíme:
		$$ S_n^2 = \frac{1}{n-1} \sum_{k=1}^n (X_k - \mu - (\overline{X_n} - \mu))^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \mu)^2 - 2 \frac{1}{n-1} \underbrace{\sum_{i=1}^n (X_i - \mu) (\overline{X_n} - \mu)}_{= (\overbrace{X_n} -\mu) \sum_{i=1}^n (X_i - \mu) · \frac{n}{n} = (\overbrace{X_n} -\mu) · ((\overbrace{X_n} -\mu)) · n} + \underbrace{\frac{1}{n-1} \sum_{i = 1}^n (\overline{X_n} - \mu)^2}_{= n (\overline{X_n} - \mu)^2} = $$
		$$ = \frac{1}{n - 1} \sum_{i=1}^n (X_i -\mu)^2 - \frac{n}{n-1}(\overbrace{X_n} -\mu)^2. $$

		Nestrannost $S_n^2$:
		$$ ®E S_n^2 = ®E \(\frac{1}{n-1} (X_i -\mu)^2 - \frac{n}{n-1}(\overbrace{X_n} -\mu)^2\) = \frac{1}{n - 1} \sum_{i=1}^n \var X_k - \frac{n}{n-1} \var \overline{X_n} = \frac{n}{n - 1}·\sigma^2 - \frac{n}{n-1}·\frac{\sigma^2}{n} = \sigma^2. $$

		Konzistence $S_n^2$:
		$$ S_n^2 = \frac{1}{n - 1} \sum_{i=1}^n (X_i -\mu)^2 - \frac{n}{n-1}(\overbrace{X_n} -\mu)^2 = \frac{n}{n - 1} · \frac{1}{n}\sum_{i=1}^n (X_i -\mu)^2 - \frac{n}{n-1}(\overbrace{X_n} -\mu)^2 \rightarrow 1 · ®E(X_1 - \mu)^2 - 1 · 0 = \sigma^2 $$
		skoro jistě ze silného zákona velkých čísel a silného zákona velkých čísel spojeného s větou o transformaci.
	\end{dukazin}
\end{veta}

\begin{definice}[Výběrový moment]
	Výběrový moment je definován jako $\widehat{m_r(\theta)} := \frac{1}{n} \sum_{i=1}^n X_i^r$.
\end{definice}

\begin{tvrzeni}[Platí]
	Výběrové momenty jsou nestranné a silné konzistentní odhady $m_r(\theta)$.

	\begin{dukazin}
		Domácí úkol.
	\end{dukazin}
\end{tvrzeni}

\begin{definice}[Metoda momentů]
	Najdeme $\hat\theta$ jako řešení rovnic $m_r(\theta) = \widehat{m_r(\theta)}$.
\end{definice}

% 02. 05. 2022

% Probíral se jen příklad na odhady

% 04. 05. 2022

\section{Intervalový odhad}
\begin{definice}[(Oboustranný) intervalový odhad]
	Buď $¦X = (X_1, …, X_n)$ náhodný výběr o rozsahu $n$ z $P_\theta$ a buď $\alpha \in (0, 1)$. (Oboustranným) intervalovým odhadem parametrické funkce $g(\theta)$ o spolehlivosti $(1 - \alpha)$ nazveme dvojici borelovských funkcí $\eta_L, \eta_U: ®R^n \rightarrow ®R$, jejichž předpis nezávisí na $\theta$ a $\forall \theta \in \Theta$ platí
	$$ P_\theta(\eta_L(¦X) < g(\theta) < \eta_U(¦X)) ≥ 1 - \alpha. $$
\end{definice}

\begin{definice}[Horní a dolní oboustranný odhad]
	Buď ¦X náhodný výběr z $P_\theta$ o rozsahu $n$ a buď $\alpha \in (0, 1)$. Dolním intervalovým odhadem parametrické funkce $g(\theta)$ o spolehlivosti $(1 - \alpha)$ nazveme borelovskou funkci $\eta_D: ®R^n \rightarrow ®R$, jejíž předpis nezávisí na $\theta$ a $\forall \theta \in \Theta$ platí
	$$ P_\theta(\eta_D(¦X) < g(\theta)) ≥ 1 - \alpha. $$
	Horním intervalovým odhadem $g(\theta)$ o spolehlivosti $(1 - \alpha)$ nazveme borelovskou funkci $\eta_H: ®R^n \rightarrow ®R$ jejíž předpis nezávisí na $\theta$ a $\forall \theta \in \Theta$ platí
	$$ P_\theta(g(\theta) < \eta_H(¦X)) ≥ 1 - \alpha. $$
\end{definice}

\begin{definice}[$\beta$-kvantil]
	Buď $F$ distribuční funkce spojitá a ryze rostoucí na $F^{-1}(0, 1)$ a $\beta \in (0, 1)$. $\beta$-kvantil rozdělení s distribuční funkcí $F$ nazveme $q_\beta = F^{-1}(\beta)$.
\end{definice}

% 09. 05. 2022

\subsection{Intervalové odhady v normálním modelu}
\begin{veta}
	Buď $¦X = (X_1, …, X_n)^T$ náhodný výběr z $N(\mu, \sigma^2)$, kde $\theta = \mu \in ®R$ je neznámý parametr a $\sigma^2 > 0$ je známá konstanta. A buď $\alpha \in (0, 1)$. Pak
	$$ (\overline{X_n} - u_{1 - \frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}, \overline{X_n} + u_{1 - \frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}) $$
	je intervalový odhad $\mu$ o spolehlivosti $(1 - \alpha)$.

	\begin{dukazin}
		Postupujeme podle obecného postupu odvození intervalového odhadu:

		Volíme $H(¦X, \mu) = \sqrt{n} \frac{\overline{X_n} - \mu}{\sigma} \sim N(0, 1)$. Pak
		$$ P_\mu(u_{\frac{\alpha}{2}} < H(¦X, \mu) < u_{1 - \frac{\alpha}{2}}) = \Phi(u_{1 - \frac{\alpha}{2}}) - \Phi(u_{\frac{\alpha}{2}}) = 1 - \frac{\alpha}{2} - \frac{\alpha}{2} = 1 - \alpha, \qquad \forall \mu \in ®R\ \forall \alpha \in (0, 1). $$
		Ekvivalentní úpravou (za pomoci $u_{\frac{\alpha}{2}} = u_{1 - \frac{\alpha}{2}}$) upravíme na:
		$$ P_\mu(\overline{X_n} - u_{1 - \frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} < \mu < \overline{X_n} + u_{1 - \frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}). $$
	\end{dukazin}
\end{veta}

\begin{veta}[O vlastnostech $\overline{X_n}$ a $S_n^2$ v normálním modelu]
	Buď $¦X = (X_1, …, X_n)^T$ náhodný výběr z $N(\mu, \sigma^2)$, $\mu \in ®R$, $\sigma^2 > 0$. (Oba neznáme.) Pak

	\begin{enumerate}
		\item $\overline{X_n}$ a $S_n^2 = \frac{1}{n - 1} \sum_{i=1}^n (X_i - \overline{X_n})^2$ jsou nezávislé;
		\item náhodná veličina $\frac{n-1}{\sigma^2} S_n^2 = \frac{\sum_{i=1}^n (X_i - \overline{X_n})^2}{\sigma^2} \sim \chi^2_{n - 1}$;
		\item náhodná veličina $T_n = \frac{\overline{X_n} - \mu}{S_n} \sqrt{n} \sim t_{n-1}$ (Studentovo $t$-rozdělení o $(n-1)$ stupních volnosti, $S_n := \sqrt{S_n^2}$).
	\end{enumerate}

	\begin{dukazin}
		Zatím vynecháme. (Pravděpodobně úplně vynecháme.)
	\end{dukazin}
\end{veta}

\begin{veta}
	Buď $¦X = (X_1, …, X_n)^T$ náhodný výběr z rozdělení $N(\mu, \sigma^2)$, kde $\mu \in ®R$, $\sigma^2 \in ®R^+$ jsou neznámé parametry. Buď $\alpha \in (0, 1)$. Pak

	\begin{itemize}
		\item $(\overline{X_n} - t_{n-1}(1 - \frac{\alpha}{2}) \frac{S_n}{\sqrt{n}}, \overline{X_n} + t_{n-1}(1 - \frac{\alpha}{2}) \frac{S_n}{\sqrt{n}})$ je intervalový odhad parametru $\mu$ o spolehlivosti $(1 - \alpha)$;
		\item $\(\frac{(n - 1)S_n^2}{\chi_{n-1}^2 (1 - \frac{\alpha}{2})}, \frac{(n - 1)S_n^2}{\chi_{n-1}^2 (\frac{\alpha}{2})}\)$ je intervalový odhad parametru $\sigma^2$ o spolehlivosti $(1 - \alpha)$.
	\end{itemize}

	\begin{poznamkain}
		$\chi_{n-1}^2(\beta)$ je $\beta$-kvantil $\chi^2$-rozdělení o $(n-1)$ stupních volnosti a $t_{n-1}(\beta)$ je $\beta$-kvantil Studentova $t_{n-1}$ rozdělení.
	\end{poznamkain}

	\begin{dukazin}
		Podle obecného postupu odvození intervalového odhadu. Volíme $H(¦X, \mu, \sigma^2) = \frac{\overline{X_n} - \mu}{\sqrt{S_n^2}} \sqrt{n} \sim t_{n-1}$ (z třetího bodu předchozí věty). 
		$$ P_{\mu, \sigma^2}(t_{n - 1}(\frac{\alpha}{2}) < \frac{\overline{X}_n - \mu}{S_n}\sqrt{n} < t_{n-1}(1 - \frac{\alpha}{2})) = 1 - \alpha, \qquad \forall \alpha \in (0, 1)\ \forall \mu \in ®R\ \forall \sigma^2 > 0. $$
		Upravíme nerovnosti:
		$$ P\(\overline{X_n} - t_{n-1}(1 - \frac{\alpha}{2}) \frac{S_n}{\sqrt{n}} < \mu < \overline{X_n} + t_{n-1}(1 - \frac{\alpha}{2}) \frac{S_n}{\sqrt{n}}\) = 1 - \alpha. $$

		Obecným postupem odvození intervalového odhadu dostaneme ze statistiky $H(¦X, \mu, \sigma^2) = \frac{(n-1)S_n^2}{\sigma^2}$ druhý intervalový odhad:
		$$ P_{\mu, \sigma^2}(…) = F_{\chi_{n-1}^2}(\chi_{n-1}^2(1 - \frac{\alpha}{2})) - F_{\chi_{n-1}^2}(\chi_{n-1}^2(\frac{\alpha}{2})) = 1 - \frac{\alpha}{2} - \frac{\alpha}{2} = 1 - \alpha. $$
	\end{dukazin}
\end{veta}

\subsection{Intervalové odhady založené na CLV}
\begin{definice}[Intervalový odhad s asymptotickou spolehlivostí]
	Buďte $¦X = (X_1, …, X_n)^T$ náhodná veličina z $P_\theta$, $\theta \in \Theta$. Pokud $\eta_L(¦X)$ a $\eta_U(¦X)$ splňuje
	$$ \lim_{n \rightarrow ∞} P_\theta(\eta_L(¦X) < g(\theta) < \eta_U(¦X)) = 1 - \alpha, $$
	pak mluvíme o intervalovém odhadu s asymptotickou spolehlivostí $1 - \alpha$.
\end{definice}

% 16. 05. 2022

\begin{veta}
	Buď $¦X = (X_1, …, X_n)$ náhodný výběr z rozdělení, jež je prvkem modelu $©F = \{P_\theta, \theta \in \Theta\}$, ve kterém platí $\var_\theta X_1 \in ®R^+$. Buď $\alpha \in (0, 1)$. Potom intervalovým odhadem střední hodnoty $\mu = E_\theta X$ o asymptotické spolehlivosti $(1 - \alpha)$ je interval $(\overline{X}_n - u_{1 - \frac{\alpha}{2}} \frac{S_n}{\sqrt{n}}, \overline{X}_n + u_{1 - \frac{\alpha}{2}} \frac{S_n}{\sqrt{n}})$.

	\begin{dukazin}
		Za domácí úkol (použije se standardní postup odvození intervalového odhadu se statistikou $H(¦X, \theta) = \frac{\overline{X_n} - \mu}{\sqrt{S_n^2}} \sqrt{n}$).
	\end{dukazin}
\end{veta}

\section{Testování hypotéz}
\begin{poznamka}
	1. Formulujeme statistický model: $(X_1, …, X_n) \sim P_{\theta_0} \in ©F = \{P_\theta | \theta \in \Theta\}$.

	2. Formulujeme nulovou a alternativní hypotézu, tedy $\Theta = \Theta_0 \cup \Theta_1$ disjunktní, taková, že $\theta \in \Theta_0$ znamená, že $\theta$ je uspokojivé, a $\theta \in \Theta_1$ znamená, že $\theta$ je problematická (klasicky ta hypotéza, kterou testujeme). Mluvíme o tom, že testujeme nulovou hypotézu $H_0: \theta \in \Theta_0$ proti alternativní hypotéze $H_1: \theta \in \Theta_1$.

	3. Volíme hladinu testu $\alpha$ = maximální pravděpodobnost chyby prvního druhu ($H_0$ zamítnuta i když platí). Většinou $\alpha = 0.01$.

	4. Volba rozhodovacího pravidla / kritického oboru $W \subset ®R^n$, tj. $¦X \in W \implies$ zamítám $H_0$, $¦X \notin W \implies$ nezamítám $H_1$.

	5. Až teď! Provedeme experiment.
\end{poznamka}

\end{document}
