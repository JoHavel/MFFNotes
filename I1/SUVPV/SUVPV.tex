\documentclass[12pt]{article}					% Začátek dokumentu
\usepackage{../../MFFStyle}					    % Import stylu



\begin{document}
\section{Úvod}
    Bude se pracovat v Matlabu, v moodle je skupina.

% 7. 10. 2020

    \begin{poznamka}[Úkoly počítačového vidění]
        Detekovat, najít a určit věci (tváře, jestli se smějí, nádory, znaky jako znaky na SPZ, biometrika jako oko, tvář, podpis, budovy, lidi, auta, …) na obrázku
    \end{poznamka}

    \begin{poznamka}[Rozpoznávání objektu tradičním náhledem]
        pixely -> určení feature (např. rozdělení oblastí podle tvarů, důležitých bodů, barev, umístění hledaného objektu) expertem -> učení klasifikátoru -> rozpoznávání
    \end{poznamka}
    
    \begin{definice}[Feature vektor]
        Univerzální převedení obrázku do důležitých věcí.    

        Měl by být invariantní (měl by být stejný při rotaci a škálování), diskriminační (dobře rozdělovat objekty), kompaktní (co nejmenší)
    \end{definice}

    \begin{definice}[Rozpoznávání]
        Feature vektory tvoří prostor, kde se algoritmus naučí najít hranici, která odděluje objekty, co jsou nějaké a co jsou jinaké.
    \end{definice}

    \begin{poznamka}[Klasifikace může být za pomocí]
        Statistiky -- Bayesova teorie rozhodování

        Pravidel -- Rozhodovací strom

        Metriky -- Technika nejbližšího souseda, diskriminační analýza?, podpůrné vektorové stroje?

        Biologické inspirace -- Neuronové sítě
    \end{poznamka}

    \begin{definice}[Učení s učitelem]
        Na training setu víme správné odpovědi.
    \end{definice}

    \begin{definice}[Naivní Bayesův klasifikátor]
        Vychází z podmíněné pravděpodobnosti na základě věcí, co víme.
        \begin{prikladyin}
            Rozeznávání falešného úsměvu. 91,3\%.
        \end{prikladyin}
    \end{definice}

    \begin{definice}[Rozhodovací stromy]
        Pravidly určíme, kterou větví se vydáme. Výhodou je, že nepotřebujeme koncept vzdálenosti.
        \begin{prikladyin}
            Rozpoznávání, co se děje na videu (např. vražda). 70\% - 100\%.
        \end{prikladyin}
    \end{definice}
    
    \begin{definice}[K nejbližších sousedů]
        Podíváme se na nejbližší známé objekty a rozhodneme se podle nich.
        \begin{prikladyin}
            Čtení znaků. 99\% čísla, 94\% velká a 89\% malá písmena.
        \end{prikladyin}
    \end{definice}

    \begin{definice}[Lineární klasifikace]
        Rozdělení prostoru nadrovinou. Zlepšením je tzv. podpůrné vektorové stroje? (support vector machines)
        \begin{prikladyin}
            Rozpoznávání lidí a věku. 66,9 - 80\% lidi, 63,8 - 75,7\% věk.
        \end{prikladyin}
    \end{definice}

    \begin{definice}[Umělé neuronové sítě]
        Sítě z neuronů, které jsou velmi jednoduše simulovány, viz moje maturitní práce. (Na GitHubu pod uživatelem JoHavel).
        \begin{prikladyin}
            Rozpoznávání tváře. 90\%. (80\% na portrétech.)
        \end{prikladyin}
    \end{definice}

    \begin{poznamka}[Hluboké učení]
        pixely -> učení se včetně feature -> rozpoznávání

        \begin{prikladyin}
            AlexNet (top 5 error cca. 16\%)

            Každoročně se pořádá ILSVRC (Imagenet Large Scale Visual Recognition Challenge), kde už se dosáhlo méně než 4\% chyby (152 vrstev NN)…
        \end{prikladyin}
    \end{poznamka}

    \begin{poznamka}[Kombinovaný přístup]
        pixely -> featury nalezené NN -> trénování klasifikátoru -> rozpoznávání

        Hluboké učení se ale zdá účinnější.
    \end{poznamka}

% 12. 10. 2020 Vyučování nebylo z důvodu výpadku proudu

% 19. 10. 2020
    
    \begin{definice}[Klasifikační pipeline]
        Features -> {Výběr featur, jejich normalizace, …} -> klasifikace (výběr klasifikátoru, trénování klasifikátoru a následná klasifikace) -> evaluace -> features (respektive výstup, pokud jsme spokojeni).
    \end{definice}

    \begin{definice}[Features]
        Vlastnosti objektu, spojují nějakým způsobem podobné objekty. Musí být diskriminativní (pokud nejsou dostatečně diskriminativní, jakože často nejsou, dá se ještě hledat rozdělení s nejmenší chybou). Měly by být kompaktní (co nejmenší, protože s příliš featurami nelze ve stejném čase dostatečně naučit klasifikátor).
    \end{definice}

    \begin{definice}[Normalizace feature]
        Aby nenastával problém např. s odlišnými jednotkami, nebo s různě naškálovanými featury, normalizuje se vydělením referenční hodnotou, respektive různými statistickými metodami, např. standardizací $\(\tilde x_i = \frac{x_i - \mu}{\sigma}\)$, nebo $3\sigma$ škálováním $\(\tilde x_i = \frac{\frac{x_i - \mu}{3\sigma}+1}{2}\)$.
    \end{definice}

    \begin{poznamka}
        Rozhodovací stromy (a náhodné lesy), naivní bayesova metoda atd. nepotřebují normalizaci.
    \end{poznamka}

    \begin{definice}[Výběr featur]
        Některé featury můžou být totožné, některé zas redudantní.

        Takže vybereme nějakou podmnožinu featur a vyzkoušíme. Nebo se naopak podíváme na jednotlivé, ohodnotíme je a vybereme $K$ nejlepších. Další možnost je přidávat je po jedné a testovat je ne samotné, ale s již vybranými. Nebo můžeme začít se všemi a odstraňovat nejhorší (zase oběma způsoby, sekvenčním $1$ i jednokrokovým $K$ ). Existuje i kombinovaný, který udělá nejdříve jedno a pak druhé. Pak existují i genetické a další algoritmy.

        Podle čeho měřit: konzistence (jestli shodné hodnoty jsou ve shodné třídě, viz vzorec v prezentaci), nezávislost na ostatních featurách (opak tzv. korelace) + korelace s třídami, množství informace ($©I = -\log(P(A = a_i))$, $E(©I) = -\sum P(A = a)·\log_2(P(A=a))$), co nám dá, vzdálenost mezi třídami po použití dané featury, …
    \end{definice}

% 29. 10. 2020

    \begin{definice}[Transformace featur]
        Unsupervised (minimalizována je ztráta informací): Principal Component Analysis (PCA), Latent Semantic Indexing (LSI), Independent Component Analysis (ICA), …

        Supervised (maximalizuje se vzdálenost mezi třídami): Linear Discriminant Analysis (LDA), Canonical Correlation Analysis (CCA), Partial Least Squares (PLS), …
    \end{definice}

    \begin{definice}[PCA]
        Také Kaurhunen-Loeve (K-L) method. Hledá v rotacích a deformacích os největší varianci ($b_1^T \Sigma b_1$, kde $b_1$ je vektor projekce, $\Sigma$ je matice kovariance). Maximum se najde Lagrangeovy multiplikátory jako místo, kde $b^T_1 \Sigma b_1 = \lambda$, kde $\lambda$ je vlastní číslo $\Sigma$, je největší. Pro druhý vektor dostaneme totéž.

        $$ \Sigma = \frac{1}{N}XX^T (X =\text{ rozdíl od průměru}) $$ 
        $$ \Sigma b_j = \lambda b_j (b_j = \text{ vlastní vektory}) $$
        $$ X' = B^TX (B = \[b_j\])$$ 
    \end{definice}

    \begin{definice}[SVD (singular value decomposition)]
        Hledáme $USV^T$ tak, že $U$ a $V$ jsou vlastní vektory $A^TA$ a $AA^T$ a $V^T$ je poté diagonální matice vlastních čísel.

        $$ Y = \frac{1}{\sqrt{N}}X^T \implies Y^TY = \Sigma $$
        $$ Y = USV^T $$
        $$ V = \text{ vlastní čísla matice } Y^TY = \Sigma $$
    \end{definice}

    \begin{definice}[ICA]
        Báze nejsou kolmé. Je potřeba aby data byla nezávislá, tedy vycentrujeme odečtením průměru, a vyčistíme tím, že vynásobíme odmocninami vlastních čísel. Počítá se přes entropii (viz prezentace).
    \end{definice}

    \begin{poznamka}
        Unsupervised transformace featur může vést ke ztrátě oddělení tříd.
    \end{poznamka}

    \begin{definice}[LDA]
        Snažíme se dostat průměr každé třídy co nejdále od průměru všeho.

%        $\omega_j$ jsou prvky třídy $j$.
        Hodně vzorců, viz prezentace. Hledáme tolikarozměrnou projekční nadrovinu (v prezentaci její báze označena $w^T$), kolik máme tříd - 1. 
    \end{definice}

    \begin{poznamka}
        Lze použít nejdříve LDA, abychom snížili rozměry a následně použít LDA.
    \end{poznamka}

    \begin{poznamka}
        Na LDA potřebujeme hodně tréninkových dat. I při hodně dat jsou situace, kdy PCA je lepší (hlavně, když se třídy překrývají).
    \end{poznamka}


\end{document}
