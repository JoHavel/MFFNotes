\documentclass[12pt]{article}					% Začátek dokumentu
\usepackage{../../MFFStyle}					    % Import stylu



\begin{document}

% 17. 02. 2022

\section*{Organizační úvod}
\begin{poznamka}[Organizační úvod]
	Zápočet není nutný na zkoušku. Zkouška bude mít dvě části: první bude problém, který máme nějak vyřešit pomocí probraných algoritmů, druhá pak 2 otázky z teorie. Streemování nebude, budou pravděpodobně přednášky z minulého roku (asi na Teams).
\end{poznamka}

\section{Úvod}
\begin{definice}[AI]
	Umělou inteligenci máme hlavně dvou druhů, symbolickou (pracuje nad symbolicky (formálním jazykem – např. matematickou logikou) popsaným světem, řeší například plánování a reprezentaci znalostí) a výpočetní (pracuje přímo s „reálným světem“ = daty, například ML (NN, DL, k-means, stromy), evoluční algoritmy a další přírodou inspirované algoritmy).

	Symbolická AI může být třeba (klasicky taková AI má počáteční stav a akci se jménem, předpokladem, přidáním efektů a ubráním efektů):
	\begin{verbatim}
on(B, A)
on(table, C)
clear(A), clear(C)

pick(X)
	predpoklad: clear(X), on(A, X)
	+efekty: holding(X), clear(A)
	-efekty: clear(X), on(A, X)
	\end{verbatim}
\end{definice}

\begin{poznamka}
	Dále jsme si povídali o jednoduchých základech NN.
\end{poznamka}

\begin{definice}[Evoluční algoritmy]
	Řeší nějakou optimalizaci (hledání minima/maxima). Funguje tak, že máme nějakou populaci bodů, nějakým způsobem je křížíme + mutujeme a udržujeme velikost populace odebráním horších členů (hodnocení členů se nazývá fitness).
\end{definice}

\begin{definice}[Učení s učitelem, učení bez učitele, zpětnovazebné učení]
	Učení s učitelem je, že máme zadaná nějaká data i s výsledky (dělí se na klasifikaci = předpověď kategorie a regresi = předpověď „spojitého“ čísla).

	Učení bez učitele není, „že bych vás tu teď opustil a učte se sami“, ale že nejsou dané správné odpovědi.

	Zpětnovazebné učení je, když cílem agenta je maximalizovat nějakou zpětnou vazbu z prostředí (většinou vyjadřovanou jako číslo, kladné je „odměna“, záporná „trest“).
\end{definice}

% 24. 02. 2022

\section{Zpětnovazebné učení}
\begin{definice}[Mountain Car („autíčko v ďolíčku“)]
	Auto je v 2D údolí a nemá výkon na to, aby vyjelo nahoru přímo. Cílem je samozřejmě dostat se nahoru (pomocí akcí dopředu, dozadu, neutral). Odměna je $-1$ za každý krok v prostředí (před dojetím do cíle).

	(Existuje i spojitější verze, kde akce -- reálné číslo mezi $-1$ a $1$ -- udává zrychlení)
\end{definice}

\begin{definice}[Zpětnovazebné učení]
	Ve zpětnovazebném učení máme nějakého agenta, který provádí akce v prostředí a dostává informaci o stavu a odměně (ta se často dá spočítat ze stavu, ale pro jednoduchost rozlišujeme stav a odměnu). Formálně: Agent dostává stav $s_t$ a provede akci $a_t$.
\end{definice}

\begin{definice}
	Podobně jako v AIUvod máme spojité a diskrétní a deterministické a nedeterministické prostředí.
\end{definice}

\subsection{Markovské rozhodovací procesy}
\begin{definice}[Markovský rozhodovací proces]
	Markovský rozhodovací proces je čtveřice $(S, A, P, R)$, kde $S$ je množina stavů, $A$ je množina akcí (občas to bývá funkce ze stavů -- v každém stavu lze provést různé akce), $P_a(s, s')$ je přechodová funkce -- pravděpodobnost, že aplikací $a \in A$ v $s \in S$ přejde prostředí do $s' \in S$, $R_a(s, s')$ je odměna, kterou dostane agent při přechodu z $s \in S$ do $s' \in S$ pomocí $a \in A$. $P$ splňuje markovskou podmínku, tj. nezávisí na historii, závisí opravdu jen na $a, s, s'$.
\end{definice}

\begin{definice}[Strategie (policy)]
	Chování agenta popisujeme pomocí strategie $\pi(s, a)$, což je pro každé $s$ pravděpodobnostní distribuce akcí.
\end{definice}

\begin{definice}[Diskontovaná odměna]
	Cílem je maximalizovat odměnu (přes různé volby $\pi$): $\sum_{t=0}^∞ \gamma^t R_{a_t}(s_t, s_{t+1})$, kde $a_t = \pi(s_t)$ je akce provedená agentem v kroku $t$ a $\gamma < 1$ je diskontní faktor, který zajišťuje, že suma konverguje, nastavuje, jak moc je důležité získat odměny co nejdřív, …

	$V^\pi(s) = E[R] = E[\sum_{t = 0}^∞ \gamma^t r_t | s_0 = s]$, kde $R$ je diskontovaná odměna, $r_t$ je odměna získaná v čase $t$. $Q^\pi(s, a)$ je očekávaná odměna, kterou dostaneme, když ve stavu $s$ uděláme akci $a$ a budeme pokračovat dál strategií $\pi$, říkáme tomu hodnota akce $a$ ve stavu $s$.

	Cílem agenta je tedy najít optimální strategii $\pi^*$ takovou, že maximalizuje $V^{\pi}$. Hodnotu stavů a akcí pro optimální strategii budeme značit s $*$ místo $\pi$.
\end{definice}

\begin{definice}[$\epsilon$-greedy strategie]
	S pravděpodobností $1 - \epsilon$ vybere nejlepší akci (podle známých ohodnocení) a s pravděpodobností $\epsilon$ zvolí náhodnou akci.
\end{definice}

\begin{definice}[Monte-Carlo metody]
	Pro výpočet $V^\pi(s)$ odsimulujeme $n$-krát budoucnost a zprůměrujeme odměny.
\end{definice}

\begin{definice}[Temporal-difference metody]
	TD metody upravují ohodnocení stavů $V(s) \leftarrow V(s) - \alpha(r + \gamma V(s') - V(s))$.
\end{definice}

\begin{definice}[Q-učení]
	Q-učení funguje podobně jako temporal-difference metody, jen upravují $Q$ místo $V$, a to pomocí toho, že $V(s) = \max_a Q(s, a)$. Tradičně je $Q$ reprezentováno jako matice, která je na začátku nulová a následně se upravuje podle:
	$$ Q(s_t, a_t) \leftarrow (1 - \alpha)·Q(s_t, a_t) + \alpha·\(r_t + \gamma · \max_a Q(s_{t+1}, a)\). $$

	\begin{poznamkain}
		Lze si všimnout, že tu není potřeba znát pravděpodobnosti.
	\end{poznamkain}
\end{definice}

\begin{definice}[SARSA]
	Skoro jako Q-učení, jen místo maxima se kouká o krok dále a používá aktuální strategii agenta.
\end{definice}

% 03. 03. 2022

\section{Evoluční algoritmy}
\begin{definice}[OneMAX]
	OneMAX je problém na prostoru $\{0, 1\}^N$, kde chceme dosáhnout nějaký (neznámý) pattern, např. samé jedničky. Fitness funkce bude počet správných prvků, např. počet jedniček.
\end{definice}

\begin{definice}[Evoluční algoritmus násvosloví]
	Jeden „krok“ se jmenuje generace, jedinci použití na výrobu jiného se nazývají jeho rodiče, on se nazývá potomek.
\end{definice}

\begin{definice}[Genetický algoritmus]
	Genetický algoritmus je evoluční algoritmus, který funguje na problému kódovaném jedničkami a nulami.
	
	Máme nějakou populaci jedinců. Na nich provedeme selekci (pomocí fitness), pak křížení (nejčastěji jednobodové, tedy že od jednoho bodu prohodíme posloupnost jedinců, ale může být i uniformní, tedy že vyberu u každého bodu náhodně). Následuje mutace, kde náhodně přehodíme bity jedinců a pak začínáme od znova.

	Selekce může být ruletová (pravděpodobnost výběru jedince je fitness jedince dělená součtem fitness všech, předpokladem je fitness $≥$ 0).
\end{definice}

\begin{definice}[Elitismus]
	Občas se při náhodné selekci nenáhodně vyberou do dalšího kroku dva nejlepší a nevyberou dva nejhorší jedinci.
\end{definice}

\end{document}
