\documentclass[12pt]{article}					% Začátek dokumentu
\usepackage{../../MFFStyle}					    % Import stylu



\begin{document}

% 05. 10. 2022
\begin{poznamka}[Note of Me -- autor of notes]
	Bad English in this text is my fault, not lecturer's one.
\end{poznamka}

\section*{Úvod}
\begin{poznamka}
	3 part exam: theorem -> proof; scientific paper -> understand + explain; therms + concepts -> explain

	credits: homework (time demanding)

	Microsoft teams
\end{poznamka}

\subsection{Matrix analysis / linear algebra}
\begin{poznamka}
	Scalar product: $¦u, ¦v \in ®R^3$: $¦u·¦v$, cross product: $¦u \times ¦v$, and more: $¦u = u^i ¦e_i$ $¦u·¦v = \delta_{ij}(u^i v^j)$, $(¦u \times ¦v)_i = \epsilon_{ijk}u_jv_k$ (where $\epsilon_{ijk}$, Levi-Civita symbol, does expecting thing).
\end{poznamka}

\begin{definice}[Tensor product]
	$$ ¦u \otimes ¦v \qquad (¦u \otimes ¦v)¦w := ¦u(¦v·¦w) $$
\end{definice}

\begin{tvrzeni}[Identities for Levi-Civita symbol]
	$$ \epsilon_{ijk}\epsilon_{lmn} = \det \begin{pmatrix} \delta_{il} & \delta_{im} & \delta_{in} \\ \delta_{jl} & \delta_{jm} & \delta_{jn} \\ \delta_{kl} & \delta_{km} & \delta_{kn} \end{pmatrix} $$
	$$ \epsilon_{ijk}·\delta_{lm} = \epsilon_{jkm}·\delta_{il} + \epsilon_{klm}·\delta_{jl} + \epsilon_{ijm}·\delta_{kl} $$
	$$ \epsilon_{ijk}\epsilon_{imn} = \delta_{jm} \delta_{kn} - \delta_{jn} \delta_{km} $$
	$$ \epsilon_{ijm}\epsilon_{ijn} = 2\delta_{mn} $$
\end{tvrzeni}

\begin{definice}[Transpose matrix]
	$®A \in ®R^{3 \times 3}$, $®A^T$ is defined as $\forall ¦u, ¦v \in ®R^3: ®A^T ¦u · ¦v := ¦u·®A ¦v$.
\end{definice}

\begin{definice}[Trace of matrix]
	$  ®A \in ®R^{3 \times 3}$, $\tr ®A$ is defined as $\tr (¦u \times ¦v) = ¦u·¦v$.
\end{definice}

\begin{poznamka}
	Matrix, tensor and linear operator is the same.

	$$ ®A = A_{ij} ¦e_i \otimes ¦e_j, \qquad ®A ¦v = (A_{ij} ¦e_i \otimes ¦e_j)(v_m¦e_m) = A_{ij} v_m ¦e_i(¦e_j · ¦e_m) = (A_{ij}v_j) ¦e_i. $$
\end{poznamka}

\begin{definice}[Axial vector]
	$®A \in ®R^{3 \times 3}$, ®A is stew-symetric ($-®A = ®A^T$). Then we can prove that $\forall ¦w \in ®R^3: ®A ¦w = ¦v_{®A} \times ¦w$. We call $¦v$ the axial vector.

	\begin{poznamkain}
		$¦v_{®A} = (A_{23}, A_{13}, A_{12})^T$.
	\end{poznamkain}
\end{definice}

\begin{tvrzeni}
	$®A ¦v_{®A} = ¦o$ and $(¦u \otimes ¦v)^T = (¦v \otimes ¦u)$.
\end{tvrzeni}

\begin{definice}[Determinant in 3D]
	$\det ®A := \frac{®A ¦u · (®A ¦v \times ®A ¦w)}{¦u·(¦v \times ¦w)} $ for three arbitrary vectors $¦u, ¦v, ¦w \in ®R^3$.
\end{definice}

\begin{poznamka}[Nanson formula]
	$$ ¦w · (¦u \times ¦v) = (\det ®A)^{-1} ®A¦w · (®A¦u \times ®A ¦v) = ¦w · (\det ®A)^{-1} ®A^T (®A ¦u \times ®A ¦v) \implies $$
	$$ \implies ¦u \times ¦v = (\det A)^{-1} ®A^T (®A¦u \times ®A ¦v) $$
	$$ ®A ¦u \times ®A ¦v = (\det ®A) ®A^{-T} (¦u \times ¦v) $$
\end{poznamka}

\begin{definice}[Cofactor]
	$$ \cof ®A := (\det ®A)®A^{-T}. $$

	(Change of surface area under linear mapping ®A.)
\end{definice}

\begin{definice}[Eigenvalues, eigenvectors]
	$®A ¦v = \lambda ¦v$.

	Characteristic polynomial: $\det(®A - \mu ®I) = - \mu^3 + c_1\mu^2 - c_2 \mu + c_3$.
\end{definice}

\begin{veta}[Cayley-Hamilton]
	$$ -®A^3 + c_1®A^2 - c_2 ®A + c_3®I = ®O $$
\end{veta}

\begin{tvrzeni}
	$$ c_3 = \lambda_1·\lambda_2·\lambda_3 = \det ®A $$
	$$ c_2 = \lambda_1 \lambda_2 + \lambda_1 \lambda_3 + \lambda_2 \lambda_3 = \tr \cof ®A = \frac{1}{2} ((\tr ®A)^2 - \tr(®A^2)) $$
	$$ c_1 = \lambda_1 + \lambda_2 + \lambda_3 = \tr ®A $$

	\begin{dukazin}
		With definition of characteristic polynomial, Cayley-Hamilton and Schur decomposition. Schur decomposition: $®A \in ®R^{3 \times 3}$. There exists an invertible matrix ®U and upper triangular matrix ®T such that
		$$ ®A = ®U^{-1}®T®U, \qquad ®T = \begin{pmatrix} \lambda_1 & T_{12} & T_{13} \\ 0 & \lambda_2 & T_{23} \\ 0 & 0 & \lambda_3 \end{pmatrix}. $$
	\end{dukazin}
\end{tvrzeni}

\begin{tvrzeni}[Useful identity from CH]
	$$ ®A^{-1} = \frac{1}{c_3}®A^2 - \frac{c_1}{c_3} ®A + \frac{c_2}{c_3}®I = \frac{1}{\det ®A}®A^2 - \frac{\tr ®A}{\det ®A} ®A + \frac{\tr \cof ®A}{\det ®A} ®I $$
\end{tvrzeni}

\begin{poznamka}[Functions of matrices]
	$\exp ®A$, $\ln ®A$, $\sin ®A$, …

	There are several ways of define it: Analytics calculus = Taylor series, Borel calculus: $®A = \sum \lambda_i ¦v_i \otimes ¦v_i$ () $\implies$ $f(®A) := \sum f(\lambda_i) ¦v_i \otimes ¦v_i$, Holomorphic calculus ($f(z) = \frac{1}{2 \pi i} \int_{\gamma} \frac{f(\zeta)}{(\zeta - z)} d\zeta$) $f(®A) = \frac{1}{2 \pi i} \int_{\gamma} f(\zeta)(\zeta ®I - ®A)^{-1} d\zeta$ (where curve $\zeta$ envelops eigenvalues of $®A$)
\end{poznamka}

\begin{tvrzeni}[Useful identities for functions]
	$$ \det(\exp ®A) = \exp(\tr ®A) $$
	$$ \exp ®A = \lim_{n \rightarrow ∞}\(®I + \frac{®A}{n}\)^n $$
\end{tvrzeni}

% 12. 10. 2022

\begin{definice}[Invariants of matrix]
	$$ \lambda_1 + \lambda_2 + \lambda_3 = \tr ®A = I_1; $$
	$$ \lambda_1 \lambda_2 + \lambda_1 \lambda_3 + \lambda_2 \lambda_3 = \tr \cof ®A = \frac{1}{2} ((\tr ®A)^2 - \tr(®A^2)) = I_2; $$
	$$ \lambda_1·\lambda_2·\lambda_3 = \det ®A = I_3 $$
\end{definice}

\subsection{Representation theorems for isotropic functions}
\begin{definice}[Isotropic function]
	$\phi: ®R^{3 \times 3} \rightarrow ®R$ is isotropic $≡$ $\phi(®Q ®A ®Q^T) = \phi(®A)$ for all proper orthogonal matrices ($®Q®Q^T = ®I$, $\det ®Q > 0$).

	$f: ®R^{3 \times 3} \rightarrow ®R^{3 \times 3}$ is isotropic $≡$ $f(®Q ®A ®Q^T) = ®Q f(®A) ®Q^T$ for all proper orthogonal matrices.
\end{definice}

\begin{veta}
	A scalar function $\phi: ®A \in ®R^{3 \times 3} \rightarrow ®R$ of symmetric matrices is isotropic if and only if it can be rewritten as a function of invariants of ®A.
\end{veta}

\begin{veta}
	A matrix valued function $f: ®A \in ®R^{3 \times 3} \rightarrow ®R^{3 \times 3}$ (from symmetric matrices to symmetric matrices) is isotropic if and only if it can be rewritten as
	$$ f(®A) = \alpha_0 ®I + \alpha_1 ®A + \alpha_2 ®A^2, $$
	where $\{\alpha_i\}_{i=1}^3$ are scalar function of the invariants.
\end{veta}

\begin{dusledek}
	$®A \mapsto ®A^{-1}$ is isotropic function.
\end{dusledek}

\begin{poznamka}[Notation]
	$$ ®A, ®B \in ®R^{3 \times 3}, \qquad ®A:®B := \tr(®A ®B^T), \qquad ||®A|| := (\tr (®A ®A^T))^{1 / 2} $$
\end{poznamka}

\subsection{Calculus}
\begin{definice}[Gateaux derivative]
	$$ Df(x)[y] = \(\frac{d}{d\tau} f(x + \tau y)\)|_{\tau=0}. $$
\end{definice}

\begin{definice}[Fréchet derivative]
	$$ \lim_{||y|| \rightarrow 0} \frac{||f(x + y) - f(x) - D f(x)[y]||}{||y||} = 0. $$
\end{definice}

\begin{poznamka}
	$Df(®A)[®B] \sim \frac{\partial f}{\partial ®A}(®A)[®B] \sim \frac{\partial f}{\partial ®A}(®A) : ®B$.
\end{poznamka}

\begin{priklad}
	$$ D(I_2(®A))[B] = D(-\frac{1}{2} \tr ®A^2 + \frac{1}{2} (\tr ®A)^2)[B] = \frac{d}{d\tau} (-\frac{1}{2} \tr (®A + \tau ®B)^2 + \frac{1}{2} (\tr (®A + \tau®B))^2)|_{\tau = 0} = $$
	$$ = - \tr(®A®B) + (\tr ®A)(\tr ®B) = (\tr ®A)®I : ®B - ®A^T:®B = ((\tr ®A)®I - ®A^T):®B. $$

	$$ D(\det ®A)[®B] = \frac{d}{d\tau}(\det (®A + \tau ®B))|_{\tau = 0} = (\det ®A) \frac{d}{d\tau}(\det(®I + \tau ®A^{-1}®B))_{\tau=0} = \det ®A \frac{d}{d\tau}(1 + \tau \tr (®A^{-1} ®B) + …)|_{\tau=0} = (\det ®A) \tr(®A^{-1}®B) = (\det ®A)®A^{-T}:®B. $$
\end{priklad}

\begin{poznamka}
	Chain rule works as usual.
\end{poznamka}

\begin{priklad}
	$$ \frac{d}{dt}(\det ®A(t)) = (\det ®A) \tr\(®A^{-1} \frac{d®A}{dt}\). $$
\end{priklad}

\begin{priklad}
	$$ \frac{\partial ®A^{-1}}{\partial ®A}[®B] = \frac{d}{d \tau}\((®A + \tau ®B)^{-1}\)|_{\tau = 0} = \frac{d}{d \tau}\((®I + \tau ®A^{-1} ®B)^{-1}®A^{-1}\)|_{\tau = 0} = \frac{d}{d\tau}\((®I - \tau ®A^{-1}®B + …)®A^{-1}\)|_{\tau = 0} = - ®A^{-1}®B®A^{-1}. $$
\end{priklad}

\begin{priklad}
	$$ \frac{\partial e^{®A}}{\partial ®A}[®B] = \frac{d}{d \tau}\(e^{®A + \tau ®B}\)|_{\tau = 0} = \frac{d}{d\tau} \(®I + (®A + \tau ®B) + \frac{(®A + \tau ®B)^2}{2!} + …\)|_{\tau = 0} = $$
	$$ = \frac{d}{d\tau}\(®I + (®A \tau ®B) + … + \tau(®A ®B + ®B ®A) + \tau(®A®A®B + ®A®B®A + ®B®A®A) + …\) $$
\end{priklad}

\begin{veta}[Daleckii-Krein theorem]
	$®A \in ®R^{3 \times 3}$ real symmetric matrix. $®A = \sum_{i=1}^3 \lambda_i ®P_i$, $®P_i$-projector to $i$-th eigenvector, $®P_i = ¦v_i \otimes ¦v_i$. $f$ real valued function $f: ®R \rightarrow ®R$ differentiable.
	$$ f(®A) := \sum_{i=1}^3 f(\lambda_i) ®P_i = \sum_{i=1}^3 f(\lambda_i) ¦v_i \otimes ¦v_i. $$
	$$ Df(®A)[®B] = \sum_{i=1}^3 \frac{df}{d\lambda}|_{\lambda = \lambda_i} ®P_i ®B ®P_i + \sum_{i=1}^3 \sum_{j=1, j≠i}^3 \frac{f(\lambda_i) - f(\lambda_j)}{\lambda_i - \lambda_j} ®P_i ®B ®P_j $$
	$$ (Df(®A)[®B])_{ij} = \frac{f(\lambda_i) - f(\lambda_j)}{\lambda_i - \lambda_j} B_{ij}, if i ≠ j, (Df(®A)[®B])_{ij} = \frac{df}{d\lambda}|_{\lambda = \lambda_j} B_{ij}, if i=j. $$

	\begin{dukazin}
		From chain rule:
		$$ \frac{\partial f(®A)}{\partial ®A} = \sum_{i=1}^3 \frac{df(\lambda_i)}{d\lambda} |_{\lambda = \lambda_i} \frac{\partial \lambda_i}{\partial ®A} ¦v_i \otimes ¦v_i + \sum_{i=1}^3 f(\lambda_i) \frac{\partial ¦v_i}{\partial ®A}\otimes ¦v_i + \sum_{i=1}^3 f(\lambda_i) ¦v_i \otimes \frac{\partial ¦v_i}{\partial ®A} $$

		First derivative at right side:
		$$ ®A ¦v_i = \lambda_i¦v $$
		$$ \frac{\partial ®A}{\partial ®A} ¦v_i + ®A \frac{\partial ¦v_i}{\partial ®A} = \frac{\partial \lambda_i}{\partial ®A}¦v_i + \lambda_i \frac{\partial ¦v_i}{\partial ®A} \qquad ·¦v_i $$
		$$ \frac{\partial A_{mn}}{\partial A_{kl}}(¦v_i)_n + (A_{mn}) \frac{\partial (¦v_i)_n}{\partial A_{kl}} = \frac{\partial \lambda_i}{\partial A_{kl}}(¦v_i)_m + \lambda_i \frac{\partial (¦v_i)_n}{\partial A_{kl}} $$
		$$ \delta_{mk}\delta_{nl}(¦v_i)_n + A_{mn}\frac{\partial (¦v_i)_n}{\partial A_{kl}} = \frac{\partial \lambda_i}{\partial A_{kl}}(¦v_i)_m + \lambda_i \frac{\partial (¦v_i)_n}{\partial A_{kl}} \qquad ·(¦v_i)_m \sum_m $$

		$$ \sum_m \frac{\partial \lambda_i}{\partial A_{kl}}(¦v_i)_m (¦v_i)_m = \frac{\partial \lambda_i}{\partial A_{kl}} $$

		From symmetry of ®A and definition of eigenvector:
		$$ \sum_m A_{mn}\frac{\partial (¦v_i)_n}{\partial A_{kl}} (¦v_i)_m = \lambda_i \frac{\partial (¦v_i)_n}{\partial A_{kl}}(¦v_i)_n $$

		$$ \sum_m \delta_{mk}\delta_{nl}(¦v_i)_n (¦v_i)_m = \delta_{nl}(¦v_i)_n(¦v_i)_k $$
		
		So
		$$ \lambda_i \frac{\partial (¦v_i)_n}{\partial A_{kl}}(¦v_i)_n + \delta_{nl}(¦v_i)_n(¦v_i)_k = \frac{\partial \lambda_i}{\partial A_{kl}} + \lambda_i \frac{\partial (¦v_i)_n}{\partial A_{kl}} \qquad \sum_n $$
		$$ \sum_n \lambda_i \frac{\partial (¦v_i)_n}{\partial A_{kl}}(¦v_i)_n + (¦v_i)_l(¦v_i)_k = \frac{\partial \lambda_i}{\partial A_{kl}} + \lambda_i \frac{\partial (¦v_i)_n}{\partial A_{kl}} $$
		$$ (¦v_i)_l(¦v_i)_k = \frac{\partial \lambda_i}{\partial A_{kl}} $$
		$$ \frac{\partial \lambda_i}{\partial ®A} = ¦v_i \otimes ¦v_j $$

		Second derivative at right side:
		$$ \frac{\partial ®A}{\partial ®A} ¦v_i + ®A \frac{\partial ¦v_i}{\partial ®A} = \frac{\partial \lambda_i}{\partial ®A}¦v_i + \lambda_i \frac{\partial ¦v_i}{\partial ®A} \qquad ·¦v_j $$
		$$ ®A \frac{\partial ¦v_i}{\partial ®A}·¦v_j = \lambda_i \frac{\partial ¦v_i}{\partial ®A}·¦v_j $$
		…
		$$ \frac{\partial ¦v_i}{\partial ®A}·¦v_j = \frac{¦v_j \otimes ¦v_i}{\lambda_i - \lambda_j}i = \frac{\delta_{kj}\delta_{il}}{\lambda_i - \lambda_j} $$
		$$ \(\frac{\partial ¦v_i}{\partial ®A}[®X]\)_j = \frac{\delta_{im}\delta_{jn}}{\lambda_i - \lambda_j} = \frac{¦v_i · ®X ¦v_j}{\lambda_i - \lambda_j}. $$

		$$ \frac{\partial ¦v_i}{\partial ®A}[®X] = \sum_{j=1}^3 \frac{¦v_i · ®X¦v_j}{\lambda_i - \lambda_j}¦v_j. $$
	\end{dukazin}
\end{veta}

\begin{poznamka}[V dokončení důkazu se ještě použije]
	$$ (¦a \otimes ¦b):(¦c \otimes ¦d) = (¦b·¦c)(¦a·¦d). $$
\end{poznamka}

\end{document}
